[
{
	"uri": "/calico/install_calico/",
	"title": "Instalar Calico",
	"tags": [],
	"description": "",
	"content": "Aplique o manifesto Calico do projeto GitHub aws/amazon-vpc-cni-k8s . Isso cria o daemon set no namespace kube-system .\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.2/calico.yaml  Vamos ver alguns dos principais recursos do manifesto Calico:\n1) Nós vemos uma annotation throughout; annotations são uma maneira de anexar metadados non-identifying para objetos. Estes metadados não são usados ​​internamente pelo Kubernetes, então eles não podem ser usados ​​para serem identificados dentro do K8. Em vez disso, eles são usados ​​por ferramentas e bibliotecas externas. Exemplos de anotações incluem timestamps de compilação/liberação, informações da biblioteca do cliente para depuração ou campos gerenciados por uma política de rede, como o Calico, neste caso.\nkind: DaemonSet apiVersion: extensions/v1beta1 metadata: name: calico-node namespace: kube-system labels: k8s-app: calico-node spec: selector: matchLabels: k8s-app: calico-node updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: k8s-app: calico-node annotations: # This, along with the CriticalAddonsOnly toleration below, # marks the pod as a critical add-on, ensuring it gets # priority scheduling and that its resources are reserved # if it ever gets evicted. *scheduler**.alpha.kubernetes.io/critical-pod: ''* ...  Em contraste, Labels no Kubernetes devem ser usados ​​para especificar atributos identifying para objetos. Eles são usados ​​por consultas de seletor ou com seletores de labels. Como eles são usados ​​internamente pelo Kubernetes, a estrutura de chaves e valores é restrita para otimizar as consultas.\n2) Nós vemos que o manifesto tem um atributo de tolerância. Contornos e tolerâncias trabalhe em conjunto para garantir que os pods não sejam agendados em nós inapropriados. As impurezas são aplicadas aos nós e apenas pods que podem tolerar a contaminação podem ser executados nesses nós.\nUma contaminação consiste em uma chave, um valor para isso e um efeito, que pode ser:\n PreferNoSchedule: Prefiro não agendar pods intolerantes para o nó contaminado NoSchedule: Não agende pods intolerantes para o nó contaminado NoExecute: Além de não agendar, também despeje os pods intolerantes que já estão em execução no nó.   Como as contaminações, as tolerâncias também têm um par de valores-chave e um efeito, com a adição do operador. Aqui no manifesto Calico, vemos tolerâncias tem apenas um atributo: Operator = exists. Isso significa que o par de valores-chave é omitido e a tolerância corresponderá a qualquer contaminação, garantindo que seja executada em todos os nós.\n tolerations: - operator: Exists  Observe os conjuntos de daemon do kube-system e aguarde até que o daemon do nó calico esteja configurado para ter o número DESIRED de pods no estado READY.\nkubectl get daemonset calico-node --namespace=kube-system  Expected Output:\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE calico-node 3 3 3 3 3 \u0026lt;none\u0026gt; 38s  "
},
{
	"uri": "/calico/stars_policy_demo/create_resources/",
	"title": "Criar recursos",
	"tags": [],
	"description": "",
	"content": "Antes de criar políticas de rede, vamos criar os recursos necessários. Crie uma nova pasta para os arquivos de configuração.\nmkdir ~/environment/calico_resources cd ~/environment/calico_resources  ####Namespace Stars\nCopie/Cole os seguintes comandos no seu terminal Cloud9.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/namespace.yaml  Vamos examinar nosso arquivo executando cat namespace.yaml.\nkind: Namespace apiVersion: v1 metadata: name: stars  Crie um namespace chamado stars:\nkubectl apply -f namespace.yaml  Vamos criar frontend e backend controladores de replicação e services neste namespace em etapas posteriores.\nCopie/Cole os seguintes comandos no seu terminal Cloud9.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/management-ui.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/backend.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/frontend.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/client.yaml  cat management-ui.yaml:\napiVersion: v1 kind: Namespace metadata: name: management-ui labels: role: management-ui --- apiVersion: v1 kind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 80 targetPort: 9001 selector: role: management-ui --- apiVersion: v1 kind: ReplicationController metadata: name: management-ui namespace: management-ui spec: replicas: 1 template: metadata: labels: role: management-ui spec: containers: - name: management-ui image: calico/star-collect:v0.1.0 imagePullPolicy: Always ports: - containerPort: 9001  Criar um namespace management-ui , com um serviço do namespace management-ui e um controlador de replicação dentro desse namespace:\nkubectl apply -f management-ui.yaml  cat backend.yaml para ver como o serviço de back-end é construído:\napiVersion: v1 kind: Service metadata: name: backend namespace: stars spec: ports: - port: 6379 targetPort: 6379 selector: role: backend --- apiVersion: v1 kind: ReplicationController metadata: name: backend namespace: stars spec: replicas: 1 template: metadata: labels: role: backend spec: containers: - name: backend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=6379 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 6379  Vamos examinar o serviço de frontend com cat frontend.yaml:\napiVersion: v1 kind: Service metadata: name: frontend namespace: stars spec: ports: - port: 80 targetPort: 80 selector: role: frontend --- apiVersion: v1 kind: ReplicationController metadata: name: frontend namespace: stars spec: replicas: 1 template: metadata: labels: role: frontend spec: containers: - name: frontend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=80 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 80  Crie controladores e serviços de replicação frontend e backend dentro do namespace stars:\nkubectl apply -f backend.yaml kubectl apply -f frontend.yaml  Por fim, vamos examinar como o namespace client e um serviço de client para um controlador de replicação. são construídos. cat client.yaml:\nkind: Namespace apiVersion: v1 metadata: name: client labels: role: client --- apiVersion: v1 kind: ReplicationController metadata: name: client namespace: client spec: replicas: 1 template: metadata: labels: role: client spec: containers: - name: client image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status ports: - containerPort: 9000 --- apiVersion: v1 kind: Service metadata: name: client namespace: client spec: ports: - port: 9000 targetPort: 9000 selector: role: client  Aplique a configuração do cliente.\nkubectl apply -f client.yaml  Verifique seu status e aguarde até que todos os pods alcancem o status de execução:\n$ kubectl get pods --all-namespaces  Sua saída deve ficar assim:\nNAMESPACE NAME READY STATUS RESTARTS AGE client client-nkcfg 1/1 Running 0 24m kube-system aws-node-6kqmw 1/1 Running 0 50m kube-system aws-node-grstb 1/1 Running 1 50m kube-system aws-node-m7jg8 1/1 Running 1 50m kube-system calico-node-b5b7j 1/1 Running 0 28m kube-system calico-node-dw694 1/1 Running 0 28m kube-system calico-node-vtz9k 1/1 Running 0 28m kube-system calico-typha-75667d89cb-4q4zx 1/1 Running 0 28m kube-system calico-typha-horizontal-autoscaler-78f747b679-kzzwq 1/1 Running 0 28m kube-system kube-dns-7cc87d595-bd9hq 3/3 Running 0 1h kube-system kube-proxy-lp4vw 1/1 Running 0 50m kube-system kube-proxy-rfljb 1/1 Running 0 50m kube-system kube-proxy-wzlqg 1/1 Running 0 50m management-ui management-ui-wzvz4 1/1 Running 0 24m stars backend-tkjrx 1/1 Running 0 24m stars frontend-q4r84 1/1 Running 0 24m  Pode levar vários minutos para baixar todas as imagens necessárias do Docker.\n Para resumir os diferentes recursos que criamos:\n Um namespace chamado stars frontend e backend controladores de replicação e serviços dentro do namespace stars Um namespace chamado management-ui Controlador e serviço de replicação management-ui para a interface do usuário visto no navegador, No namespace management-ui Um namespace chamado client client controlador de replicação e serviço no namespace client  "
},
{
	"uri": "/prerequisites/self_paced/account/",
	"title": "Crie uma conta da AWS",
	"tags": [],
	"description": "",
	"content": "Sua conta deve ter a capacidade de criar novas roles do IAM e escopo de outras permissões do IAM.\n  Se você ainda não tem uma conta da AWS com acesso de administrador: crie uma agora clicando aqui\n Depois de ter uma conta da AWS, verifique se você está seguindo as etapas restantes da oficina como um usuário do IAM com acesso de administrador à conta da AWS: Criar um novo usuário do IAM para usar no workshop\n Digite os detalhes do usuário:  Anexe a Política do IAM AdministratorAccess:  Clique para criar o novo usuário:  Anote o URL de login e salve:   "
},
{
	"uri": "/conclusion/conclusion/",
	"title": "O que construímos",
	"tags": [],
	"description": "",
	"content": "We have:\n Implantou um aplicativo que consiste em microsserviços Implantou o Dashboard do Kubernetes Pacotes implantados usando o Helm Implantou uma infraestrutura de logs centralizada Escala automática configurada de nossos pods e worker nodes  "
},
{
	"uri": "/",
	"title": "Workshop Amazon EKS ",
	"tags": [],
	"description": "",
	"content": "Amazon EKS Workshop Neste workshop, vamos explorar várias maneiras de configurar VPC, ALB e EC2 workers do Kubernetes e Amazon Elastic Container Service para Kubernetes.\n"
},
{
	"uri": "/calico/stars_policy_demo/default_policy/",
	"title": "Comunicação padrão Pod-to-Pod",
	"tags": [],
	"description": "",
	"content": "No Kubernetes, os pods por padrão pode se comunicar com outros pods, independentemente de qual host eles se comunicam. Cada pod recebe seu próprio endereço IP para que você não precise criar explicitamente links entre os pods. Isso é demonstrado pelo management-ui.\nkind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 80 targetPort: 9001  Para abrir a interface do usuário de gerenciamento usando, recupere o nome DNS da interface do usuário de gerenciamento usando:\nkubectl get svc -o wide -n management-ui  Copie o EXTERNAL-IP da saída e cole em um navegador. A coluna EXTERNAL-IP contém um valor que termina com \u0026ldquo;elb.amazonaws.com” - o valor total é o endereço DNS.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR management-ui LoadBalancer 10.100.239.7 a8b8c5f77eda911e8b1a60ea5d5305a4-720629306.us-east-1.elb.amazonaws.com 80:31919/TCP 9s role=management-ui  A interface do usuário aqui mostra o comportamento padrão, de todos os serviços sendo capazes de alcançar um ao outro.\n"
},
{
	"uri": "/calico/stars_policy_demo/apply_network_policies/",
	"title": "Aplicar políticas de rede",
	"tags": [],
	"description": "",
	"content": " Em um cluster de nível de produção, não é seguro ter uma comunicação aberta de pod para pod. Vamos ver como podemos isolar os serviços uns dos outros.\nCopie/Cole os seguintes comandos no seu terminal Cloud9.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/default-deny.yaml  Vamos examinar nosso arquivo executando cat default-deny.yaml.\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny spec: podSelector: matchLabels: {}  Vamos repassar a política de rede. Aqui vemos que o podSelector não tem nenhum matchLabels, essencialmente bloqueando todos os pods de acessá-lo.\nAplique a política de rede no namespace stars (serviços frontend e backend) e o namespace client (client service):\nkubectl apply -n stars -f default-deny.yaml kubectl apply -n client -f default-deny.yaml  Ao atualizar seu navegador, você vê que a Interface de gerenciamento não pode alcançar nenhum dos nós, portanto, nada aparece na interface do usuário.\nAs políticas de rede no Kubernetes usam rótulos para selecionar pods e definir regras sobre o tráfego permitido para alcançar esses pods. Eles podem especificar entrada ou saída ou ambos. Cada regra permite o tráfego que corresponde às seções de e portas.\nCrie duas novas políticas de rede.\nCopie/Cole os seguintes comandos no seu terminal Cloud9.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/allow-ui.yaml wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/allow-ui-client.yaml  Mais uma vez, podemos examinar o conteúdo dos nossos arquivos executando: cat allow-ui.yaml\nkind: NetworkPolicy apiVersion: extensions/v1beta1 metadata: namespace: stars name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui  cat allow-ui-client.yaml\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: client name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui  Desafio: Como aplicamos nossas políticas de rede para permitir o tráfego que desejamos?\n  Expanda aqui para ver a solução   kubectl apply -f allow-ui.yaml kubectl apply -f allow-ui-client.yaml    Ao atualizar seu navegador, você pode ver que a interface do usuário de gerenciamento pode alcançar todos os serviços, mas eles não podem se comunicar uns com os outros.\n"
},
{
	"uri": "/calico/stars_policy_demo/directional_traffic/",
	"title": "Permitir tráfego direcional",
	"tags": [],
	"description": "",
	"content": " Vamos ver como podemos permitir o tráfego direcional do cliente para frontend e backend.\nCopie/Cole os seguintes comandos no seu terminal Cloud9.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/directional_traffic.files/backend-policy.yaml wget https://eksworkshop.com/calico/stars_policy_demo/directional_traffic.files/frontend-policy.yaml  Vamos examinar essa política de back-end com cat backend-policy.yaml:\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELEST FRONTEND USING PODSELECTOR\u0026gt; \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELEST FRONTEND USING PODSELECTOR\u0026gt; \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELEST FRONTEND USING PODSELECTOR\u0026gt; ports: - protocol: TCP port: 6379  Challenge: Depois de analisar o manifesto, você verá que deixamos intencionalmente alguns dos campos de configuração para você EDITAR. Por favor, edite a configuração conforme sugerido. Você pode encontrar informações úteis nesta Documentação do kubernetes\n  Expanda aqui para ver a solução   kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379    Vamos examinar a política de frontend comcat frontend-policy.yaml:\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELEST CLIENT USING NAMESPACESELECTOR\u0026gt; \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELEST CLIENT USING NAMESPACESELECTOR\u0026gt; \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELEST CLIENT USING NAMESPACESELECTOR\u0026gt; ports: - protocol: TCP port: 80  Desafio: Por favor, edite a configuração conforme sugerido. Você pode encontrar informações úteis nesta Documentação do kubernetes\n  Expanda aqui para ver a solução   kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80    Para permitir o tráfego do serviço de frontend para o serviço de back-end, aplique o seguinte manifesto:\nkubectl apply -f backend-policy.yaml  E permitir tráfego do namespace do cliente para o serviço frontend:\nkubectl apply -f frontend-policy.yaml  Ao atualizar seu navegador, você poderá ver as políticas de rede em ação:\nVamos dar uma olhada na política de backend. Sua especificação tem um podSelector que seleciona todos os pods com a label role:backend, e permite a entrada de todos os pods que têm o label role:frontend e na porta TCP 6379, mas não o contrário. O tráfego é permitido em uma direção em um número de porta específico.\nspec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379  O frontend-policy é semelhante, exceto pelo fato de permitir a entrada de ** namespaces ** que têm o label role: client na porta TCP 80.\nspec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80  "
},
{
	"uri": "/monitoring/prereqs/",
	"title": "Prereqs",
	"tags": [],
	"description": "",
	"content": " Is helm installed? We will use helm to install Prometheus \u0026amp; Grafana monitoring tools for this chapter. Please review installing helm chapter for instructions if you don\u0026rsquo;t have it installed.\nhelm ls  Configure Storage Class We will use gp2 EBS volumes for simplicity and demonstration purpose. While deploying in Production, you would use io1 volumes with desired IOPS and increase the default storage size in the manifests to get better performance.\nSave the below manifest as prometheus-storageclass.yaml using your favorite editor.\nChallenge: You need to update provisioner value that is applicable to AWS EBS provisioner. Please see Kubernetes documentation for help\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: prometheus namespace: prometheus provisioner: \u0026lt;EDIT: UPDATE WITH VALUE OF EBS PROVISIONER\u0026gt; parameters: type: gp2 reclaimPolicy: Retain mountOptions: - debug    Expand here to see the solution   kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: prometheus namespace: prometheus provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Retain mountOptions: - debug    Challenge: Create storageclass \u0026ldquo;prometheus\u0026rdquo; by applying proper kubectl command\n  Expand here to see the solution   kubectl create -f prometheus-storageclass.yaml    "
},
{
	"uri": "/deploy/applications/",
	"title": "Faça deploy de nossos aplicativos de exemplo",
	"tags": [],
	"description": "",
	"content": "apiVersion: apps/v1 kind: Deployment metadata: name: ecsdemo-nodejs labels: app: ecsdemo-nodejs namespace: default spec: replicas: 1 selector: matchLabels: app: ecsdemo-nodejs strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: ecsdemo-nodejs spec: containers: - image: brentley/ecsdemo-nodejs:latest imagePullPolicy: Always name: ecsdemo-nodejs ports: - containerPort: 3000 protocol: TCP  No arquivo de exemplo acima, descrevemos o serviço e como deve ser implantado. Vamos escrever esta descrição para a API do kubernetes usando o kubectl, e o kubernetes irá garantir que nossas preferências sejam atendidas à medida que o aplicativo for implementado.\nOs contêineres escutam na porta 3000 e a descoberta de serviço nativo será usada para localizar os contêineres em execução e se comunicar com eles..\n"
},
{
	"uri": "/statefulset/storageclass/",
	"title": "Defenir Storageclass",
	"tags": [],
	"description": "",
	"content": " Introdução Dynamic Volume Provisioning allows storage volumes to be created on-demand. StorageClass should be pre-created to define which provisoner should be used and what parameters should be passed when dynamic provisioning is invoked. (See parameters for AWS EBS)\nDefine Storage Class Copy/Paste the following commands into your Cloud9 Terminal.\nmkdir ~/environment/templates cd ~/environment/templates wget https://eksworkshop.com/statefulset/storageclass.files/mysql-storageclass.yml  Check the configuration of mysql-storageclass.yml file by following command.\ncat ~/environment/templates/mysql-storageclass.yml  You can see provisioner is kubernetes.io/aws-ebs and type is gp2 specified as a parameter.\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: mysql-gp2 provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Delete mountOptions: - debug  Create storageclass \u0026ldquo;mysql-gp2\u0026rdquo; by following command.\nkubectl create -f ~/environment/templates/mysql-storageclass.yml  We will specify \u0026ldquo;mysql-gp2\u0026rdquo; as the storageClassName in volumeClaimTemplates at \u0026ldquo;Create StatefulSet\u0026rdquo; section later.\nvolumeClaimTemplates: - metadata: name: data spec: accessModes: [\u0026quot;ReadWriteOnce\u0026quot;] storageClassName: mysql-gp2 resources: requests: storage: 10Gi    Related files   mysql-storageclass.yml  (0 ko)    "
},
{
	"uri": "/helm_root/helm_intro/install/",
	"title": "Instalar o CLI do Helm",
	"tags": [],
	"description": "",
	"content": " Antes de começarmos a configurar helm precisaremos primeiro instalar as ferramentas de linha de comando com as quais você irá interagir. Para fazer isso, execute o seguinte.\ncd ~/environment curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh chmod +x get_helm.sh ./get_helm.sh  Depois de instalar o helm, o comando solicitará que você execute \u0026lsquo;helm init\u0026rsquo;. Do not run \u0026lsquo;helm init\u0026rsquo;. Siga as instruções para configurar o helm usando Kubernetes RBAC e, em seguida, instale o helm como especificado abaixo Se você acidentalmente executar \u0026lsquo;helm init\u0026rsquo;, você pode desinstalar com segurança o helm executando \u0026lsquo;helm reset \u0026ndash;force\u0026rsquo;\n Configurar o acesso ao Helm com o RBAC Helm conta com um serviço chamado tiller que requer permissão especial no cluster do kubernetes, então precisamos construir um Service Account para o tiller to use. We\u0026rsquo;ll then apply this to the cluster.\nPara criar um novo manifesto de conta de serviço:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system EoF  Em seguida, aplique a configuração:\nkubectl apply -f ~/environment/rbac.yaml  Então podemos instalar helm usando a ferramenta helm\nhelm init --service-account tiller  Isso instalará o tiller no cluster, que lhe dá acesso para gerenciar recursos em seu cluster.\n"
},
{
	"uri": "/calico/stars_policy_demo/cleanup/",
	"title": "Limpar",
	"tags": [],
	"description": "",
	"content": "Limpe a demonstração excluindo os namespaces:\nkubectl delete ns client stars management-ui  "
},
{
	"uri": "/healthchecks/livenessprobe/",
	"title": "Configure Liveness Probe",
	"tags": [],
	"description": "",
	"content": " Configurar o probe Use o comando abaixo para criar um diretório\nmkdir ~/environment/healthchecks  Salve o manifesto como liveness-app.yaml usando seu editor favorito. Você pode revisar o manifesto descrito abaixo. No arquivo de configuração, o livenessProbe determina como o kubelet deve verificar o contêiner para considerar se ele está em boas condições ou não. O kubelet usa o campo periodSeconds para fazer verificações freqüentes no contêiner. Nesse caso, o kubelet verifica a sonda de atividade a cada 5 segundos. O campo initialDelaySeconds é para dizer ao kubelet que ele deve esperar por 5 segundos antes de fazer o primeiro teste. Para realizar uma análise, o kubelet envia uma solicitação HTTP GET para o servidor que hospeda esse pod e, se o manipulador dos servidores / integridade retornar um código de sucesso, o Contêiner será considerado íntegro. Se o manipulador retornar um código de falha, o kubelet mata o contêiner e o reinicia.\napiVersion: v1 kind: Pod metadata: name: liveness-app spec: containers: - name: liveness image: brentley/ecsdemo-nodejs livenessProbe: httpGet: path: /health port: 3000 initialDelaySeconds: 5 periodSeconds: 5  Vamos criar o pod usando o manifesto\nkubectl apply -f ~/environment/healthchecks/liveness-app.yaml  O comando acima cria um pod com healthchecks liveness\nkubectl get pod liveness-app  A saída parece abaixo. Observe o RESTARTS\nNAME READY STATUS RESTARTS AGE liveness-app 1/1 Running 0 11s  O comando kubectl describe mostrará um histórico de eventos que mostrará qualquer falha ou reinicialização da sonda.\nkubectl describe pod liveness-app  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 38s default-scheduler Successfully assigned liveness-app to ip-192-168-18-63.ec2.internal Normal SuccessfulMountVolume 38s kubelet, ip-192-168-18-63.ec2.internal MountVolume.SetUp succeeded for volume \u0026quot;default-token-8bmt2\u0026quot; Normal Pulling 37s kubelet, ip-192-168-18-63.ec2.internal pulling image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Pulled 37s kubelet, ip-192-168-18-63.ec2.internal Successfully pulled image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Created 37s kubelet, ip-192-168-18-63.ec2.internal Created container Normal Started 37s kubelet, ip-192-168-18-63.ec2.internal Started container  Introduzir uma falha Vamos executar o próximo comando para enviar um sinal SIGUSR1 para o aplicativo nodejs. Ao emitir este comando, enviaremos um sinal de kill ao processo de aplicativo no tempo de execução do docker.\nkubectl exec -it liveness-app -- /bin/kill -s SIGUSR1 1  Descreva o pod depois de esperar por 15-20 segundos e você notará ações do Kubelet de matar o Contêiner e reiniciá-lo.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned liveness-app to ip-192-168-18-63.ec2.internal Normal SuccessfulMountVolume 1m kubelet, ip-192-168-18-63.ec2.internal MountVolume.SetUp succeeded for volume \u0026quot;default-token-8bmt2\u0026quot; Warning Unhealthy 30s (x3 over 40s) kubelet, ip-192-168-18-63.ec2.internal Liveness probe failed: Get http://192.168.13.176:3000/health: net/http: request canceled (Client.Timeout exceeded while awaiting headers) Normal Pulling 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal pulling image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Pulled 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Successfully pulled image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Created 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Created container Normal Started 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Started container Normal Killing 0s kubelet, ip-192-168-18-63.ec2.internal Killing container with id docker://liveness:Container failed liveness probe.. Container will be killed and recreated.  Quando o aplicativo nodejs entrou em um modo de depuração com sinal SIGUSR1, ele não respondeu aos pings de verificação de integridade e o kubelet interrompeu o contêiner. O contêiner estava sujeito à política de reinicialização padrão.\nkubectl get pod liveness-app  The output looks like below\nNAME READY STATUS RESTARTS AGE liveness-app 1/1 Running 1 12m  Desafio: Como podemos verificar o status das verificações de integridade do contêiner?\n  Expanda aqui para ver a solução   kubectl logs liveness-app  Você também pode usar o kubectl logs para recuperar logs de uma instanciação anterior de um container comflag --previous, caso o container tenha caído\nkubectl logs liveness-app --previous  \u0026lt;Output omitted\u0026gt; Exemplo de aplicativo ouvindo na porta 3000! ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:01 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 16 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:06 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 17 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:11 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 17 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; Starting debugger agent. Debugger listening on [::]:5858    "
},
{
	"uri": "/introduction/",
	"title": "Introdução",
	"tags": [],
	"description": "",
	"content": " Introdução ao Kubernetes Um passo a passo de conceitos básicos do Kubernetes.\nBem vindo ao Workshop Amazon EKS !\nA intenção deste workshop é educar os usuários sobre os recursos do Amazon EKS.\nConhecimentos nos fluxos de trabalho de EKS, Kubernetes, Docker e contêiner não é obrigatório, mas eles são recomendados.\nEste capítulo apresentará o funcionamento básico do Kubernetes, lançando as bases para a parte prática do workshop.\nEspecificamente, vamos orientá-lo nos seguintes tópicos:\n Kubernetes (k8s) Noções básicas   Arquitetura do Kubernetes   Amazon EKS   "
},
{
	"uri": "/batch/introduction/",
	"title": "Introdução",
	"tags": [],
	"description": "",
	"content": " Introduction Processamento em lote refere-se à execução de unidades de trabalho, referido comojob in a repetitive and unattended fashion. Jobs are typically grouped together and processed in batches (hence the name).\nKubernetes includes native support for running Jobs. Jobs can run multiple pods in parallel until receiving a set number of completions. Each pod can contain multiple containers as a single unit of work.\nArgo enhances the batch processing experience by introducing a number of features:\n Steps based declaration of workflows Artifact support Step level inputs \u0026amp; outputs Loops Conditionals Visualization (using Argo Dashboard) \u0026hellip;and more  In this module, we will build a simple Kubernetes Job, recreate that job in Argo, and add common features and workflows for more advanced batch processing.\n"
},
{
	"uri": "/servicemesh/introduction/",
	"title": "Introdução",
	"tags": [],
	"description": "",
	"content": " Istio O Istio é um service mesh completamente aberta que se espalha de forma transparente em aplicativos distribuídos existentes. É também uma plataforma, incluindo APIs, que permite a integração em qualquer plataforma de registro, telemetria ou sistema de políticas..\nVamos revisar com mais detalhes o que cada um dos componentes que compõem essa service mesh é.\n Envoy\n Processa o tráfego de entrada / saída de serviços intercalados e de serviço para serviço externo de forma transparente.  Pilot\n Pilot fornece descoberta de serviço para os sidecars Envoy, capacidades de gerenciamento de tráfego para roteamento inteligente (e.g., A/B tests, canary deployments, etc.), e resiliência (timeouts, retries, circuit breakers, etc.)  Mixer\n O Mixer aplica as políticas de controle de acesso e uso no service mesh e coleta dados de telemetria do proxy Envoy e de outros serviços.  Citadel\n O Citadel fornece uma forte autenticação entre serviço e usuário final, com gerenciamento integrado de identidade e credenciais.   "
},
{
	"uri": "/monitoring/deploy-prometheus/",
	"title": "Deploy Prometheus",
	"tags": [],
	"description": "",
	"content": " Download Prometheus curl -o prometheus-values.yaml https://raw.githubusercontent.com/helm/charts/master/stable/prometheus/values.yaml  Open the prometheus-values.yaml you downloaded by double clicking on the file name on the left panel. You need to make three edits to this file.\nSearch for storageClass in the prometheus-values.yaml, uncomment and change the value to \u0026ldquo;prometheus\u0026rdquo;. You will do this twice, under both server \u0026amp; alertmanager manifests\nThe third edit you will do is to expose Prometheus server as a NodePort. Because Prometheus is exposed as ClusterIP by default, the web UI cannot be reached outside of Kubernetes. By exposing the service as NodePort, we will be able to reach Prometheus web UI from the worker node IP address. Search for type: ClusterIP and add nodePort: 30900 and change the type to NodePort as indicated below.\nThis configuration is not recommended in Production and there are better ways to secure it. You can read more about exposing Prometheus web UI in this link\nWhen you search, you will find their are more than one type: ClusterIP in prometheus-values.yaml. You need to update relevant Prometheus manifest. See below snippet for identifying Prometheus manifest\n ## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 nodePort: 30900 type: NodePort    Expand here to see the complete yaml   rbac: create: true ## Define serviceAccount names for components. Defaults to component's fully qualified name. ## serviceAccounts: alertmanager: create: true name: kubeStateMetrics: create: true name: nodeExporter: create: true name: pushgateway: create: true name: server: create: true name: alertmanager: ## If false, alertmanager will not be installed ## enabled: true ## alertmanager container name ## name: alertmanager ## alertmanager container image ## image: repository: prom/alertmanager tag: v0.15.2 pullPolicy: IfNotPresent ## alertmanager priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## Additional alertmanager container arguments ## extraArgs: {} ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug ## so that the various internal URLs are still able to access as they are in the default case. ## (Optional) prefixURL: \u0026quot;\u0026quot; ## External URL which can access alertmanager ## Maybe same with Ingress host name baseURL: \u0026quot;/\u0026quot; ## Additional alertmanager container environment variable ## For instance to add a http_proxy ## extraEnv: {} ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.alertmanager.configMapOverrideName}} ## Defining configMapOverrideName will cause templates/alertmanager-configmap.yaml ## to NOT generate a ConfigMap resource ## configMapOverrideName: \u0026quot;\u0026quot; ingress: ## If true, alertmanager Ingress will be created ## enabled: false ## alertmanager Ingress annotations ## annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: 'true' ## alertmanager Ingress additional labels ## extraLabels: {} ## alertmanager Ingress hostnames with optional path ## Must be provided if Ingress is enabled ## hosts: [] # - alertmanager.domain.com # - domain.com/alertmanager ## alertmanager Ingress TLS configuration ## Secrets must be manually created in the namespace ## tls: [] # - secretName: prometheus-alerts-tls # hosts: # - alertmanager.domain.com ## Alertmanager Deployment Strategy type # strategy: # type: Recreate ## Node tolerations for alertmanager scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for alertmanager pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Pod affinity ## affinity: {} ## Use an alternate scheduler, e.g. \u0026quot;stork\u0026quot;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: persistentVolume: ## If true, alertmanager will create/use a Persistent Volume Claim ## If false, use emptyDir ## enabled: true ## alertmanager data Persistent Volume access modes ## Must match those of existing PV or dynamic provisioner ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## accessModes: - ReadWriteOnce ## alertmanager data Persistent Volume Claim annotations ## annotations: {} ## alertmanager data Persistent Volume existing claim name ## Requires alertmanager.persistentVolume.enabled: true ## If defined, PVC must be created manually before volume will be bound existingClaim: \u0026quot;\u0026quot; ## alertmanager data Persistent Volume mount root path ## mountPath: /data ## alertmanager data Persistent Volume size ## size: 2Gi ## alertmanager data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026quot;-\u0026quot;, storageClassName: \u0026quot;\u0026quot;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## storageClass: \u0026quot;prometheus\u0026quot; ## Subdirectory of alertmanager data Persistent Volume to mount ## Useful if the volume's root directory is not empty ## subPath: \u0026quot;\u0026quot; ## Annotations to be added to alertmanager pods ## podAnnotations: {} replicaCount: 1 ## alertmanager resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 10m # memory: 32Mi # requests: # cpu: 10m # memory: 32Mi ## Security context to be added to alertmanager pods ## securityContext: {} service: annotations: {} labels: {} clusterIP: \u0026quot;\u0026quot; ## Enabling peer mesh service end points for enabling the HA alert manager ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md # enableMeshPeer : true ## List of IP addresses at which the alertmanager service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 # nodePort: 30000 type: ClusterIP ## Monitors ConfigMap changes and POSTs to a URL ## Ref: https://github.com/jimmidyson/configmap-reload ## configmapReload: ## configmap-reload container name ## name: configmap-reload ## configmap-reload container image ## image: repository: jimmidyson/configmap-reload tag: v0.2.2 pullPolicy: IfNotPresent ## Additional configmap-reload container arguments ## extraArgs: {} ## Additional configmap-reload mounts ## extraConfigmapMounts: [] # - name: prometheus-alerts # mountPath: /etc/alerts.d # subPath: \u0026quot;\u0026quot; # configMap: prometheus-alerts # readOnly: true ## configmap-reload resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} initChownData: ## If false, data ownership will not be reset at startup ## This allows the prometheus-server to be run with an arbitrary user ## enabled: true ## initChownData container name ## name: init-chown-data ## initChownData container image ## image: repository: busybox tag: latest pullPolicy: IfNotPresent ## initChownData resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} kubeStateMetrics: ## If false, kube-state-metrics will not be installed ## enabled: true ## kube-state-metrics container name ## name: kube-state-metrics ## kube-state-metrics container image ## image: repository: quay.io/coreos/kube-state-metrics tag: v1.4.0 pullPolicy: IfNotPresent ## kube-state-metrics priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## kube-state-metrics container arguments ## args: {} ## Node tolerations for kube-state-metrics scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for kube-state-metrics pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Annotations to be added to kube-state-metrics pods ## podAnnotations: {} pod: labels: {} replicaCount: 1 ## kube-state-metrics resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 10m # memory: 16Mi # requests: # cpu: 10m # memory: 16Mi ## Security context to be added to kube-state-metrics pods ## securityContext: {} service: annotations: prometheus.io/scrape: \u0026quot;true\u0026quot; labels: {} # Exposed as a headless service: # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services clusterIP: None ## List of IP addresses at which the kube-state-metrics service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 type: ClusterIP nodeExporter: ## If false, node-exporter will not be installed ## enabled: true ## If true, node-exporter pods share the host network namespace ## hostNetwork: true ## If true, node-exporter pods share the host PID namespace ## hostPID: true ## node-exporter container name ## name: node-exporter ## node-exporter container image ## image: repository: prom/node-exporter tag: v0.16.0 pullPolicy: IfNotPresent ## Specify if a Pod Security Policy for node-exporter must be created ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/ ## podSecurityPolicy: enabled: False ## node-exporter priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## Custom Update Strategy ## updateStrategy: type: OnDelete ## Additional node-exporter container arguments ## extraArgs: {} ## Additional node-exporter hostPath mounts ## extraHostPathMounts: [] # - name: textfile-dir # mountPath: /srv/txt_collector # hostPath: /var/lib/node-exporter # readOnly: true extraConfigmapMounts: [] # - name: certs-configmap # mountPath: /prometheus # configMap: certs-configmap # readOnly: true ## Node tolerations for node-exporter scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for node-exporter pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Annotations to be added to node-exporter pods ## podAnnotations: {} ## Labels to be added to node-exporter pods ## pod: labels: {} ## node-exporter resource limits \u0026amp; requests ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 200m # memory: 50Mi # requests: # cpu: 100m # memory: 30Mi ## Security context to be added to node-exporter pods ## securityContext: {} # runAsUser: 0 service: annotations: prometheus.io/scrape: \u0026quot;true\u0026quot; labels: {} # Exposed as a headless service: # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services clusterIP: None ## List of IP addresses at which the node-exporter service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] hostPort: 9100 loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 9100 type: ClusterIP server: ## Prometheus server container name ## name: server ## Prometheus server container image ## image: repository: prom/prometheus tag: v2.4.3 pullPolicy: IfNotPresent ## prometheus server priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug ## so that the various internal URLs are still able to access as they are in the default case. ## (Optional) prefixURL: \u0026quot;\u0026quot; ## External URL which can access alertmanager ## Maybe same with Ingress host name baseURL: \u0026quot;\u0026quot; ## This flag controls access to the administrative HTTP API which includes functionality such as deleting time ## series. This is disabled by default. enableAdminApi: false global: ## How frequently to scrape targets by default ## scrape_interval: 1m ## How long until a scrape request times out ## scrape_timeout: 10s ## How frequently to evaluate rules ## evaluation_interval: 1m ## Additional Prometheus server container arguments ## extraArgs: {} ## Additional Prometheus server hostPath mounts ## extraHostPathMounts: [] # - name: certs-dir # mountPath: /etc/kubernetes/certs # subPath: \u0026quot;\u0026quot; # hostPath: /etc/kubernetes/certs # readOnly: true extraConfigmapMounts: [] # - name: certs-configmap # mountPath: /prometheus # subPath: \u0026quot;\u0026quot; # configMap: certs-configmap # readOnly: true ## Additional Prometheus server Secret mounts # Defines additional mounts with secrets. Secrets must be manually created in the namespace. extraSecretMounts: [] # - name: secret-files # mountPath: /etc/secrets # subPath: \u0026quot;\u0026quot; # secretName: prom-secret-files # readOnly: true ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}} ## Defining configMapOverrideName will cause templates/server-configmap.yaml ## to NOT generate a ConfigMap resource ## configMapOverrideName: \u0026quot;\u0026quot; ingress: ## If true, Prometheus server Ingress will be created ## enabled: false ## Prometheus server Ingress annotations ## annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: 'true' ## Prometheus server Ingress additional labels ## extraLabels: {} ## Prometheus server Ingress hostnames with optional path ## Must be provided if Ingress is enabled ## hosts: [] # - prometheus.domain.com # - domain.com/prometheus ## Prometheus server Ingress TLS configuration ## Secrets must be manually created in the namespace ## tls: [] # - secretName: prometheus-server-tls # hosts: # - prometheus.domain.com ## Server Deployment Strategy type # strategy: # type: Recreate ## Node tolerations for server scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for Prometheus server pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Pod affinity ## affinity: {} ## Use an alternate scheduler, e.g. \u0026quot;stork\u0026quot;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: persistentVolume: ## If true, Prometheus server will create/use a Persistent Volume Claim ## If false, use emptyDir ## enabled: true ## Prometheus server data Persistent Volume access modes ## Must match those of existing PV or dynamic provisioner ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## accessModes: - ReadWriteOnce ## Prometheus server data Persistent Volume annotations ## annotations: {} ## Prometheus server data Persistent Volume existing claim name ## Requires server.persistentVolume.enabled: true ## If defined, PVC must be created manually before volume will be bound existingClaim: \u0026quot;\u0026quot; ## Prometheus server data Persistent Volume mount root path ## mountPath: /data ## Prometheus server data Persistent Volume size ## size: 8Gi ## Prometheus server data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026quot;-\u0026quot;, storageClassName: \u0026quot;\u0026quot;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## storageClass: \u0026quot;prometheus\u0026quot; ## Subdirectory of Prometheus server data Persistent Volume to mount ## Useful if the volume's root directory is not empty ## subPath: \u0026quot;\u0026quot; ## Annotations to be added to Prometheus server pods ## podAnnotations: {} # iam.amazonaws.com/role: prometheus replicaCount: 1 ## Prometheus server resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 500m # memory: 512Mi # requests: # cpu: 500m # memory: 512Mi ## Security context to be added to server pods ## securityContext: {} service: annotations: {} labels: {} clusterIP: \u0026quot;\u0026quot; ## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [54.210.142.247] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 nodePort: 30900 type: NodePort ## Prometheus server pod termination grace period ## terminationGracePeriodSeconds: 300 ## Prometheus data retention period (i.e 360h) ## retention: \u0026quot;\u0026quot; pushgateway: ## If false, pushgateway will not be installed ## enabled: true ## pushgateway container name ## name: pushgateway ## pushgateway container image ## image: repository: prom/pushgateway tag: v0.5.2 pullPolicy: IfNotPresent ## pushgateway priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## Additional pushgateway container arguments ## extraArgs: {} ingress: ## If true, pushgateway Ingress will be created ## enabled: false ## pushgateway Ingress annotations ## annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: 'true' ## pushgateway Ingress hostnames with optional path ## Must be provided if Ingress is enabled ## hosts: [] # - pushgateway.domain.com # - domain.com/pushgateway ## pushgateway Ingress TLS configuration ## Secrets must be manually created in the namespace ## tls: [] # - secretName: prometheus-alerts-tls # hosts: # - pushgateway.domain.com ## Node tolerations for pushgateway scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for pushgateway pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Annotations to be added to pushgateway pods ## podAnnotations: {} replicaCount: 1 ## pushgateway resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 10m # memory: 32Mi # requests: # cpu: 10m # memory: 32Mi ## Security context to be added to push-gateway pods ## securityContext: {} service: annotations: prometheus.io/probe: pushgateway labels: {} clusterIP: \u0026quot;\u0026quot; ## List of IP addresses at which the pushgateway service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 9091 type: ClusterIP ## alertmanager ConfigMap entries ## alertmanagerFiles: alertmanager.yml: global: {} # slack_api_url: '' receivers: - name: default-receiver # slack_configs: # - channel: '@you' # send_resolved: true route: group_wait: 10s group_interval: 5m receiver: default-receiver repeat_interval: 3h ## Prometheus server ConfigMap entries ## serverFiles: alerts: {} rules: {} prometheus.yml: rule_files: - /etc/config/rules - /etc/config/alerts scrape_configs: - job_name: prometheus static_configs: - targets: - localhost:9090 # A scrape configuration for running Prometheus on a Kubernetes cluster. # This uses separate scrape configs for cluster components (i.e. API server, node) # and services to allow each to use different authentication configs. # # Kubernetes labels will be added as Prometheus labels on metrics via the # `labelmap` relabeling action. # Scrape config for API servers. # # Kubernetes exposes API servers as endpoints to the default/kubernetes # service so this uses `endpoints` role and uses relabelling to only keep # the endpoints associated with the default/kubernetes service using the # default named port `https`. This works for single API server deployments as # well as HA API server deployments. - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u0026lt;kubernetes_sd_config\u0026gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token # Keep only the default/kubernetes service endpoints for the https port. This # will add targets for each API server which Kubernetes adds an endpoint to # the default/kubernetes service. relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u0026lt;kubernetes_sd_config\u0026gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: 'kubernetes-nodes-cadvisor' # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u0026lt;kubernetes_sd_config\u0026gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node # This configuration will work only on kubelet 1.7.3+ # As the scrape endpoints for cAdvisor have changed # if you are using older version you need to change the replacement to # replacement: /api/v1/nodes/${1}:4194/proxy/metrics # more info here https://github.com/coreos/prometheus-operator/issues/633 relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor # Scrape config for service endpoints. # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/scrape`: Only scrape services that have a value of `true` # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need # to set this to `https` \u0026amp; most likely set the `tls_config` of the scrape config. # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: If the metrics are exposed on a different port to the # service then set this appropriately. - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: 'prometheus-pushgateway' honor_labels: true kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: pushgateway # Example scrape config for probing services via the Blackbox Exporter. # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/probe`: Only probe services that have a value of `true` - job_name: 'kubernetes-services' metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name # Example scrape config for pods # # The relabeling allows the actual pod scrape endpoint to be configured via the # following annotations: # # * `prometheus.io/scrape`: Only scrape pods that have a value of `true` # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`. - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # adds additional scrape configs to prometheus.yml # must be a string so you have to add a | after extraScrapeConfigs: # example adds prometheus-blackbox-exporter scrape config extraScrapeConfigs: # - job_name: 'prometheus-blackbox-exporter' # metrics_path: /probe # params: # module: [http_2xx] # static_configs: # - targets: # - https://example.com # relabel_configs: # - source_labels: [__address__] # target_label: __param_target # - source_labels: [__param_target] # target_label: instance # - target_label: __address__ # replacement: prometheus-blackbox-exporter:9115 networkPolicy: ## Enable creation of NetworkPolicy resources. ## enabled: false    Deploy Prometheus helm install -f prometheus-values.yaml stable/prometheus --name prometheus --namespace prometheus  Make a note of prometheus endpoint in helm response (you will need this later). It should look similar to below\nThe Prometheus server can be accessed via port 80 on the following DNS name from within your cluster: prometheus-server.prometheus.svc.cluster.local  Check if Prometheus components deployed as expected\nkubectl get all -n prometheus  You should see response similar to below. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/prometheus-alertmanager-77cfdf85db-s9p48 2/2 Running 0 1m pod/prometheus-kube-state-metrics-74d5c694c7-vqtjd 1/1 Running 0 1m pod/prometheus-node-exporter-6dhpw 1/1 Running 0 1m pod/prometheus-node-exporter-nrfkn 1/1 Running 0 1m pod/prometheus-node-exporter-rtrm8 1/1 Running 0 1m pod/prometheus-pushgateway-d5fdc4f5b-dbmrg 1/1 Running 0 1m pod/prometheus-server-6d665b876-dsmh9 2/2 Running 0 1m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/prometheus-alertmanager ClusterIP 10.100.89.154 \u0026lt;none\u0026gt; 80/TCP 1m service/prometheus-kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 80/TCP 1m service/prometheus-node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 1m service/prometheus-pushgateway ClusterIP 10.100.136.143 \u0026lt;none\u0026gt; 9091/TCP 1m service/prometheus-server NodePort 10.100.151.245 \u0026lt;none\u0026gt; 80/30900 1m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/prometheus-node-exporter 3 3 3 3 3 \u0026lt;none\u0026gt; 1m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/prometheus-alertmanager 1 1 1 1 1m deployment.apps/prometheus-kube-state-metrics 1 1 1 1 1m deployment.apps/prometheus-pushgateway 1 1 1 1 1m deployment.apps/prometheus-server 1 1 1 1 1m NAME DESIRED CURRENT READY AGE replicaset.apps/prometheus-alertmanager-77cfdf85db 1 1 1 1m replicaset.apps/prometheus-kube-state-metrics-74d5c694c7 1 1 1 1m replicaset.apps/prometheus-pushgateway-d5fdc4f5b 1 1 1 1m replicaset.apps/prometheus-server-6d665b876 1 1 1 1m  You can access Prometheus server URL by going to any one of your Worker node IP address and specify port :30900/targets (for ex, 52.12.161.128:30900/targets. Remember to open port 30900 in your Worker nodes Security Group. In the web UI, you can see all the targets and metrics being monitored by Prometheus\n"
},
{
	"uri": "/deploy/deploynodejs/",
	"title": "Implantar a API de back-end do NodeJS",
	"tags": [],
	"description": "",
	"content": "Vamos implantar a API do Backend do NodeJS!\nCopie/cole os seguintes comandos no seu workspace Cloud9 :\ncd ~/environment/ecsdemo-nodejs kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  Podemos acompanhar o progresso observando o status de deployment:\nkubectl get deployment ecsdemo-nodejs  "
},
{
	"uri": "/terraform/prerequisites/",
	"title": "Pré-requisitos",
	"tags": [],
	"description": "",
	"content": "Para este capítulo, precisamos baixar o binário do Terraform :\ncurl -kLo /tmp/terraform.zip \u0026quot;https://releases.hashicorp.com/terraform/0.11.8/terraform_0.11.8_linux_amd64.zip\u0026quot; sudo unzip -d /usr/local/bin/ /tmp/terraform.zip sudo chmod +x /usr/local/bin/terraform  Vamos nos certificar de que temos um binário de Terraform:\nterraform version  "
},
{
	"uri": "/spotworkers/workers/",
	"title": "Add EC2 Workers - On-Demand and Spot",
	"tags": [],
	"description": "",
	"content": " We have our EKS Cluster and worker nodes already, but we need some Spot Instances configured as workers. We also need a Node Labeling strategy to identify which instances are Spot and which are on-demand so that we can make more intelligent scheduling decisions. We will use AWS CloudFormation to launch new worker nodes that will connect to the EKS cluster.\nThis template will create a single ASG that leverages the latest feature to mix multiple instance types and purchase as a single K8s nodegroup. Check out this blog: New – EC2 Auto Scaling Groups With Multiple Instance Types \u0026amp; Purchase Options for details.\nRetrieve the Worker Role name First, we will need to collect the Role Name that is in use with our EKS worker nodes\necho $ROLE_NAME  Copy the Role Name for use as a Parameter in the next step. If you receive an error or empty response, expand the steps below to export.\n  Expand here if you need to export the Role Name   INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r .Stacks[].StackName | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') echo \u0026quot;export ROLE_NAME=${ROLE_NAME}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile    # Example Output eksctl-eksworkshop-eksctl-nodegro-NodeInstanceRole-XXXXXXXX  Launch the CloudFormation Stack We will launch the CloudFormation template as a new set of worker nodes, but it\u0026rsquo;s also possible to update the nodegroup CloudFormation stack created by the eksctl tool.\nClick the Launch button to create the CloudFormation stack in the AWS Management Console.\n   Launch template       EKS Workers - Spot and On Demand  Launch    Download      Confirm the region is correct based on where you\u0026rsquo;ve deployed your cluster.\n Once the console is open you will need to configure the missing parameters. Use the table below for guidance.\n   Parameter Value     Stack Name: eksworkshop-spot-workers   Cluster Name: eksworkshop-eksctl (or whatever you named your cluster)   ClusterControlPlaneSecurityGroup: Select from the dropdown. It will contain your cluster name and the words \u0026lsquo;ControlPlaneSecurityGroup\u0026rsquo;   NodeInstanceRole: Use the role name that copied in the first step. (e.g. eksctl-eksworkshop-eksctl-nodegro-NodeInstanceRole-XXXXX)   NodeImageId: Visit this link and select the non-GPU image for your region - Check for empty spaces in copy/paste   KeyName: SSH Key Pair created earlier or any valid key will work   VpcId: Select your workshop VPC from the dropdown   Subnets: Select the public subnets for your workshop VPC from the dropdown   BootstrapArgumentsForOnDemand: --kubelet-extra-args --node-labels=lifecycle=OnDemand   BootstrapArgumentsForSpotFleet: --kubelet-extra-args '--node-labels=lifecycle=Ec2Spot --register-with-taints=spotInstance=true:PreferNoSchedule'    What\u0026rsquo;s going on with Bootstrap Arguments? The EKS Bootstrap.sh script is packaged into the EKS Optimized AMI that we are using, and only requires a single input, the EKS Cluster name. The bootstrap script supports setting any kubelet-extra-args at runtime. We have configured node-labels so that kubernetes knows what type of nodes we have provisioned. We set the lifecycle for the nodes as OnDemand or Ec2Spot. We are also tainting with PreferNoSchedule to prefer pods not be scheduled on Spot Instances. This is a “preference” or “soft” version of NoSchedule – the system will try to avoid placing a pod that does not tolerate the taint on the node, but it is not required.\nYou can leave the rest of the default parameters as is and continue through the remaining CloudFormation screens. Check the box next to I acknowledge that AWS CloudFormation might create IAM resources and click Create\nThe creation of the workers will take about 3 minutes.\n Confirm the Nodes Confirm that the new nodes joined the cluster correctly. You should see 2-3 more nodes added to the cluster.\nkubectl get nodes  You can use the node-labels to identify the lifecycle of the nodes\nkubectl get nodes --show-labels --selector=lifecycle=Ec2Spot  The output of this command should return 2 nodes. At the end of the node output, you should see the node label lifecycle=Ec2Spot\nNow we will show all nodes with the lifecycle=OnDemand. The output of this command should return 1 node as configured in our CloudFormation template.\nkubectl get nodes --show-labels --selector=lifecycle=OnDemand  You can use the kubectl describe nodes with one of the spot nodes to see the taints aaplied to the EC2 Spot Instances.\n"
},
{
	"uri": "/logging/prereqs/",
	"title": "Configurar política do IAM para Worker Nodes",
	"tags": [],
	"description": "",
	"content": "Nós estaremos implantando o Fluentd como um DaemonSet, ou um pod node por worker node. O Daemon de Log Fluentd irá coletar logs e encaminhar para o CloudWatch Logs.. Isso exigirá que os nodes tenham permissões para enviar logs e criar grupos de log e stream de logs. Isso pode ser feito com um usuário do IAM, uma Role do IAM ou usando uma ferramenta como Kube2IAM.\nEm nosso exemplo, criaremos uma política do IAM e anexaremos a role do Worker.\nColete o perfil da instância e o NOME da role na Stack do CloudFormation\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r .Stacks[].StackName | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName')  Crie uma nova política do IAM e anexe-a a Role do Worker Node.\nmkdir ~/environment/iam_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/iam_policy/k8s-logs-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: [ \u0026quot;logs:DescribeLogGroups\u0026quot;, \u0026quot;logs:DescribeLogStreams\u0026quot;, \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker --policy-document file://~/environment/iam_policy/k8s-logs-policy.json  Valide se a política está anexada à role.\naws iam get-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker  "
},
{
	"uri": "/scaling/deploy_hpa/",
	"title": "Configure Horizontal Pod AutoScaler (HPA)",
	"tags": [],
	"description": "",
	"content": " Implantar o Metrics Server O Metrics Server é um agregador de dados de uso de recursos em todo o cluster. Essas métricas conduzirão o comportamento de dimensionamento do deployments. Vamos implantar o servidor de métricas usando o Helm configurado anteoriormente module\nhelm install stable/metrics-server \\ --name metrics-server \\ --version 2.0.4 \\ --namespace metrics  Confirme se a API de métricas está disponível. Retornar ao terminal no ambiente Cloud9\nkubectl get apiservice v1beta1.metrics.k8s.io -o yaml  Se tudo estiver bem, você deve ver uma mensagem de status semelhante à abaixo na resposta\nstatus: conditions: - lastTransitionTime: 2018-10-15T15:13:13Z message: all checks passed reason: Passed status: \u0026quot;True\u0026quot; type: Available  Agora estamos prontos para escalar um aplicativo implantado "
},
{
	"uri": "/helm_root/helm_micro/create_chart/",
	"title": "Criar um  Chart",
	"tags": [],
	"description": "",
	"content": "Os Helm charts têm uma estrutura semelhante a:\n/eksdemo /Chart.yaml # uma descrição do chart /values.yaml # padrões, podem ser substituídos durante a instalação ou atualização /charts/ # Pode conter subcharts /templates/ # os próprios arquivos de template(modelo) ...  Seguiremos esse modelo e criaremos um novo chart chamado eksdemo com os seguintes comandos:\ncd ~/environment helm create eksdemo  "
},
{
	"uri": "/statefulset/configmap/",
	"title": "Crie o ConfigMap",
	"tags": [],
	"description": "",
	"content": " Introdução ConfigMap permite dissociar artefatos de configuração e secrets do conteúdo da imagem para manter os aplicativos em contêiner portáteis. Usando o ConfigMap, você pode controlar independentemente a configuração do MySQL.\nCrie o ConfigMap Copie/Cole os seguintes comandos no seu terminal Cloud9.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/configmap.files/mysql-configmap.yml  Verifique a configuração do arquivo mysql-configmap.yml, seguindo o comando.\ncat ~/environment/templates/mysql-configmap.yml  ConfigMap armazena master.cnf, slave.cnf e passa a inicializar os pods master e slave definidos no statefulset. master.cnf é para o pod master do MySQL que possui opção de log binário(log-bin) para fornece um registro das alterações de dados a serem enviadas para servidores slaves slave.cnf com pods slaves que tem opção super-read-only .\napiVersion: v1 kind: ConfigMap metadata: name: mysql-config labels: app: mysql data: master.cnf: | # Apply this config only on the master. [mysqld] log-bin slave.cnf: | # Apply this config only on slaves. [mysqld] super-read-only  Crie o configmap \u0026lsquo;mysql-config\u0026rsquo; seguindo o comando.\nkubectl create -f ~/environment/templates/mysql-configmap.yml    Related files   mysql-configmap.yml  (0 ko)    "
},
{
	"uri": "/dashboard/dashboard/",
	"title": "Implantar o Dashboard oficial do Kubernetes",
	"tags": [],
	"description": "",
	"content": "O Dashboard oficial do Kubernetes não é implantado por padrão, mas há instruções em a documentação oficial\nPodemos implantar o Dashboard com o seguinte comando:\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml  Como isso é implantado em nosso cluster privado, precisamos acessá-lo por meio de um proxy. O Kube-proxy está disponível para fazer proxy de nossos pedidos para o serviço do dashboard. No seu workspace, execute o seguinte comando:\nkubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true \u0026amp;  Isso iniciará o proxy, escutará na porta 8080, escutará todas as interfaces e desativará a filtragem de solicitações não localhost.\nEste comando continuará sendo executado em segundo plano na sessão do terminal atual.\nEstamos desativando a filtragem de solicitações, um recurso de segurança que protege contra ataques XSRF. Isso não é recomendado para um ambiente de produção, mas é útil para nosso ambiente de desenvolvimento.\n "
},
{
	"uri": "/prerequisites/",
	"title": "Comece o workshop...",
	"tags": [],
	"description": "",
	"content": " Começando Para iniciar o workshop, siga um dos seguintes procedimentos, dependendo se você\u0026hellip;\n \u0026hellip;executando o workshop por conta própria, or \u0026hellip;participando de um evento hospedado pela AWS  Depois de concluir a configuração, continue com Crie uma seção de chave SSH\n"
},
{
	"uri": "/healthchecks/readinessprobe/",
	"title": "Configure Readiness Probe",
	"tags": [],
	"description": "",
	"content": " Configurar o probe Salve o texto do bloco a seguir como ~/environment/healthchecks/readiness-deployment.yaml. A definição readinessProbe explica como um comando linux pode ser configurado como healthcheck. Criamos um arquivo vazio */tmp/healthy para configurar o probe de prontidão e usamos o mesmo para entender como o kubelet ajuda a atualizar uma implantação apenas com pods saudáveis.\napiVersion: apps/v1 kind: Deployment metadata: name: readiness-deployment spec: replicas: 3 selector: matchLabels: app: readiness-deployment template: metadata: labels: app: readiness-deployment spec: containers: - name: readiness-deployment image: alpine command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;touch /tmp/healthy \u0026amp;\u0026amp; sleep 86400\u0026quot;] readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 3  Vamos agora criar uma implantação para testar readiness probe\nkubectl apply -f ~/environment/healthchecks/readiness-deployment.yaml  O comando acima cria uma implantação com 3 réplicas e prontidão do probe, conforme descrito no início\nkubectl get pods -l app=readiness-deployment  A saída parece semelhante ao abaixo\n NAME READY STATUS RESTARTS AGE readiness-deployment-7869b5d679-922mx 1/1 Running 0 31s readiness-deployment-7869b5d679-vd55d 1/1 Running 0 31s readiness-deployment-7869b5d679-vxb6g 1/1 Running 0 31s  Vamos também confirmar que todas as réplicas estão disponíveis para servir o tráfego quando um serviço é apontado para este deployment.\nkubectl describe deployment readiness-deployment | grep Replicas:  A saída parece abaixo\nReplicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable  Introduzir um falha Escolha um dos pods acima de 3 e emita um comando como abaixo para excluir o arquivo /tmp/healthy, o que faz com que o probe readiness falhe.\nkubectl exec -it readiness-deployment-\u0026lt;YOUR-POD-NAME\u0026gt; -- rm /tmp/healthy  readiness-deployment-7869b5d679-922mx foi escolhido em nosso cluster de exemplo. O arquivo /tmp/healthy foi excluído. Este arquivo deve estar presente para que a verificação de disponibilidade passe. Abaixo está o status após a emissão do comando.\nkubectl get pods -l app=readiness-deployment  A saída parece semelhante a abaixo:\nNAME READY STATUS RESTARTS AGE readiness-deployment-7869b5d679-922mx 0/1 Running 0 4m readiness-deployment-7869b5d679-vd55d 1/1 Running 0 4m readiness-deployment-7869b5d679-vxb6g 1/1 Running 0 4m  O tráfego não será roteado para o primeiro pod na implantação acima. A coluna pronta confirma que o probe de preparação para este pod não foi aprovado e, portanto, foi marcado como não pronto.\nAgora, verificaremos as réplicas disponíveis para atender ao tráfego quando um serviço for apontado para essa implantação.\nkubectl describe deployment readiness-deployment | grep Replicas:  A saída parece abaixo\nReplicas: 3 desired | 3 updated | 3 total | 2 available | 1 unavailable  Quando o probe de prontidão para um pod falhar, o controlador de pontos de extremidade remove o pod da lista de pontos de extremidade de todos os serviços que correspondem ao pod.\nDesafio: Como você restauraria o pod para o status Ready??   Expanda aqui para ver a solução   Execute o comando abaixo com o nome do pod para recriar o arquivo /tmp/healthy. Depois que o pod passa pela sonda, ela é marcada como pronta e começará a receber tráfego novamente.\nkubectl exec -it readiness-deployment-\u0026lt;YOUR-POD-NAME\u0026gt; -- touch /tmp/healthy  kubectl get pods -l app=readiness-deployment   \n"
},
{
	"uri": "/codepipeline/role/",
	"title": "Criar Role IAM ",
	"tags": [],
	"description": "",
	"content": "Em um CodePipeline da AWS, vamos usar o AWS CodeBuild para implantar um serviço de exemplo do Kubernetes. Isso requer uma Role Gerenciamento de identidade e acesso da AWS (IAM) capaz de interagir com o cluster EKS.\nNesta etapa, criaremos uma role do IAM e adicionaremos uma política in-line que usaremos no estágio CodeBuild para interagir com o cluster EKS por meio do kubectl.\nCrie o Role:\ncd ~/environment ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) TRUST=\u0026quot;{ \\\u0026quot;Version\\\u0026quot;: \\\u0026quot;2012-10-17\\\u0026quot;, \\\u0026quot;Statement\\\u0026quot;: [ { \\\u0026quot;Effect\\\u0026quot;: \\\u0026quot;Allow\\\u0026quot;, \\\u0026quot;Principal\\\u0026quot;: { \\\u0026quot;AWS\\\u0026quot;: \\\u0026quot;arn:aws:iam::$ACCOUNT_ID:root\\\u0026quot; }, \\\u0026quot;Action\\\u0026quot;: \\\u0026quot;sts:AssumeRole\\\u0026quot; } ] }\u0026quot; echo '{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;eks:Describe*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }' \u0026gt; /tmp/iam-role-policy aws iam create-role --role-name EksWorkshopCodeBuildKubectlRole --assume-role-policy-document \u0026quot;$TRUST\u0026quot; --output text --query 'Role.Arn' aws iam put-role-policy --role-name EksWorkshopCodeBuildKubectlRole --policy-name eks-describe --policy-document file:///tmp/iam-role-policy  "
},
{
	"uri": "/x-ray/role/",
	"title": "Modify IAM Role",
	"tags": [],
	"description": "",
	"content": "In order for the X-Ray daemon to communicate with the service, we need to add a policy to the worker nodes\u0026rsquo; AWS Identity and Access Management (IAM) role.\nModify the role in the Cloud9 terminal:\nPROFILE=$(aws ec2 describe-instances --filters --filters Name=tag:Name,Values=eksworkshop-eksctl-0-Node --query 'Reservations[0].Instances[0].IamInstanceProfile.Arn' --output text | cut -d '/' -f 2) ROLE=$(aws iam get-instance-profile --instance-profile-name $PROFILE --query \u0026quot;InstanceProfile.Roles[0].RoleName\u0026quot; --output text) aws iam attach-role-policy --role-name $ROLE --policy-arn arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess  "
},
{
	"uri": "/prerequisites/self_paced/",
	"title": "...workshop sozinho",
	"tags": [],
	"description": "",
	"content": " Executando o workshop sozinho Só preencha esta seção se você estiver executando o workshop por conta própria. Se você estiver em um evento hospedado pela AWS (como: Invent, Kubecon, Immersion Day, etc), vá para Inicie o workshop em um evento da AWS.\n  Crie uma conta da AWS   Criar uma role do IAM para sua área de trabalho   Anexe a role do IAM ao seu Workspace   "
},
{
	"uri": "/x-ray/x-ray-daemon/",
	"title": "Deploy X-Ray DaemonSet",
	"tags": [],
	"description": "",
	"content": "Now that we have modified the IAM role for the worker nodes to permit write operations to the X-Ray service, we are going to deploy the X-Ray DaemonSet to the EKS cluster. The X-Ray daemon will be deployed to each worker node in the EKS cluster. For reference, see the example implementation used in this module.\nThe AWS X-Ray SDKs are used to instrument your microservices. When using the DaemonSet in the example implementation, you need to configure it to point to xray-service.default:2000.\nThe following showcases how to configure the X-Ray SDK for Go. This is merely an example and not a required step in the workshop.\nfunc init() { xray.Configure(xray.Config{ DaemonAddr: \u0026quot;xray-service.default:2000\u0026quot;, LogLevel: \u0026quot;info\u0026quot;, }) }  To deploy the X-Ray DaemonSet:\nkubectl create -f https://eksworkshop.com/x-ray/daemonset.files/xray-k8s-daemonset.yaml  To see the status of the X-Ray DaemonSet:\nkubectl describe daemonset xray-daemon  The folllowing is an example of the command output:\nTo view the logs for all of the X-Ray daemon pods run the following\n kubectl logs -l app=xray-daemon  "
},
{
	"uri": "/codepipeline/configmap/",
	"title": "Modificar aws-auth ConfigMap",
	"tags": [],
	"description": "",
	"content": "Agora que temos o papel do IAM criado, vamos adicionar o role ao aws-auth ConfigMap para o cluster EKS.\nDepois que o ConfigMap incluir essa nova role, o kubectl no estágio CodeBuild do pipeline poderá interagir com o cluster EKS por meio da role IAM.\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) ROLE=\u0026quot; - rolearn: arn:aws:iam::$ACCOUNT_ID:role/EksWorkshopCodeBuildKubectlRole\\n username: build\\n groups:\\n - system:masters\u0026quot; kubectl get -n kube-system configmap/aws-auth -o yaml | awk \u0026quot;/mapRoles: \\|/{print;print \\\u0026quot;$ROLE\\\u0026quot;;next}1\u0026quot; \u0026gt; /tmp/aws-auth-patch.yml kubectl patch configmap/aws-auth -n kube-system --patch \u0026quot;$(cat /tmp/aws-auth-patch.yml)\u0026quot;  Se você gostaria de editar o aws-auth ConfigMap manualmente, você pode executar: $ kubectl edit -n kube-system configmap/aws-auth\n "
},
{
	"uri": "/prerequisites/aws_event/",
	"title": "...em um evento da AWS",
	"tags": [],
	"description": "",
	"content": " Executando o workshop em um evento da AWS Preencha esta seção apenas se você estiver em um evento hospedado pela AWS (tal como re:Invent, Kubecon, Immersion Day, etc). Se você estiver executando o workshop por conta própria, vá para Inicie o workshop sozinho.\n  AWS Workshop Portal   "
},
{
	"uri": "/x-ray/microservices/",
	"title": "Deploy Example Microservices",
	"tags": [],
	"description": "",
	"content": "We now have the foundation in place to deploy microservices, which are instrumented with X-Ray SDKs, to the EKS cluster.\nIn this step, we are going to deploy example front-end and back-end microservices to the cluster. The example services are already instrumented using the X-Ray SDK for Go. Currently, X-Ray has SDKs for Go, Python, Node.js, Ruby, .NET and Java.\nkubectl apply -f https://eksworkshop.com/x-ray/sample-front.files/x-ray-sample-front-k8s.yml kubectl apply -f https://eksworkshop.com/x-ray/sample-back.files/x-ray-sample-back-k8s.yml  To review the status of the deployments, you can run:\nkubectl describe deployments x-ray-sample-front-k8s x-ray-sample-back-k8s  For the status of the services, run the following command:\nkubectl describe services x-ray-sample-front-k8s x-ray-sample-back-k8s  Once the front-end service is deployed, run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser.\nkubectl get service x-ray-sample-front-k8s -o wide  After your ELB is deployed and available, open up the endpoint returned by the previous command in your browser and allow it to remain open. The front-end application makes a new request to the /api endpoint once per second, which in turn calls the back-end service. The JSON document displayed in the browser is the result of the request made to the back-end service.\nThis service was configured with a LoadBalancer so, an AWS Elastic Load Balancer (ELB) is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n When the front-end service is first deployed, it can take up to several minutes for the ELB to be created and DNS updated.\n "
},
{
	"uri": "/codepipeline/forksample/",
	"title": "Fork Repositório de exemplo",
	"tags": [],
	"description": "",
	"content": "Nós vamos agora fork o serviço Kubernetes de exemplo para que possamos modificar o repositório e acionar construções.\nFaça login no GitHub e faça um fork do serviço de exemplo em sua própria conta:\nhttps://github.com/rnzsgh/eks-workshop-sample-api-service-go\nUma vez feito o fork do repo, você pode visualizá-lo em seu GitHub repositories.\nO fork do repositório será parecido com:\n"
},
{
	"uri": "/codepipeline/githubcredentials/",
	"title": "Token de Acesso do GitHub",
	"tags": [],
	"description": "",
	"content": "Para que o CodePipeline receba retornos de chamada do GitHub, precisamos gerar um token de acesso pessoal.\nUma vez criado, um token de acesso pode ser armazenado em um código seguro e reutilizado, portanto, essa etapa só é necessária durante a primeira execução ou quando você precisar gerar novas chaves.\nAbra uma nova página de acesso pessoal no GitHub.\nVocê pode ser solicitado a digitar sua senha do GitHub\n Insira um valor para Token description, verifique o escopo de permissão repo e role para baixo e clique no botão ** Gerar token **\nCopie o token de acesso pessoal e salve-o em um local seguro para a próxima etapa\n"
},
{
	"uri": "/x-ray/x-ray/",
	"title": "X-Ray Console",
	"tags": [],
	"description": "",
	"content": "We now have the example microservices deployed, so we are going to investigate our Service Graph and Traces in X-Ray section of the AWS Management Console.\nThe Service map in the console provides a visual representation of the steps identified by X-Ray for a particular trace. Each resource that sends data to X-Ray within the same context appears as a service in the graph. In the example below, we can see that the x-ray-sample-front-k8s service is processing 39 transactions per minute with an average latency of 0.99ms per operation. Additionally, the x-ray-sample-back-k8s is showing an average latency of 0.08ms per transaction.\nNext, go to the traces section in the AWS Management Console to view the execution times for the segments in the requests. At the top of the page, we can see the URL for the ELB endpoint and the corresponding traces below.\nIf you click on the link on the left in the Trace list section you will see the overall execution time for the request (0.5ms for the x-ray-sample-front-k8s which wraps other segments and subsegments), as well as a breakdown of the individual segments in the request. In this visualization, you can see the front-end and back-end segments and a subsegment named x-ray-sample-back-k8s-gen In the back-end service source code, we instrumented a subsegment that surrounds a random number generator.\nIn the Go example, the main segment is initialized in the xray.Handler helper, which in turn sets all necessary information in the http.Request context struct, so that it can be used when initializing the subsegment.\nClick on the image to zoom\n "
},
{
	"uri": "/x-ray/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the Tracing with X-Ray module.\nThe content for this module was based on the Application Tracing on Kubernetes with AWS X-Ray blog post.\nThis module is not used in subsequent steps, so you can remove the resources now or at the end of the workshop.\nDelete the Kubernetes example microservices deployed:\nkubectl delete deployments x-ray-sample-front-k8s x-ray-sample-back-k8s kubectl delete services x-ray-sample-front-k8s x-ray-sample-back-k8s  Delete the X-Ray DaemonSet:\nkubectl delete -f https://eksworkshop.com/x-ray/daemonset.files/xray-k8s-daemonset.yaml  "
},
{
	"uri": "/codepipeline/codepipeline/",
	"title": "CodePipeline Setup",
	"tags": [],
	"description": "",
	"content": " Now we are going to create the AWS CodePipeline using AWS CloudFormation.\nCloudFormation is an infrastructure as code (IaC) tool which provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts.\nEach EKS deployment/service should have its own CodePipeline and be located in an isolated source repository.\nYou can modify the CloudFormation templates provided with this workshop to meet your system requirements to easily onboard new services to your EKS cluster. For each new service the following steps can be repeated.\nClick the Launch button to create the CloudFormation stack in the AWS Management Console.\n   Launch template       CodePipeline \u0026amp; EKS  Launch    Download      After the console is open, enter your GitHub username, personal access token (created in previous step), check the acknowledge box and then click the \u0026ldquo;Create stack\u0026rdquo; button located at the bottom of the page.\nWait for the status to change from \u0026ldquo;CREATE_IN_PROGRESS\u0026rdquo; to CREATE_COMPLETE before moving on to the next step.\nOpen CodePipeline in the Management Console. You will see a CodePipeline that starts with eks-workshop-codepipeline. Click this link to view the details.\nIf you receive a permissions error similar to User x is not authorized to perform: codepipeline:ListPipelines\u0026hellip; upon clicking the above link, the CodePipeline console may have opened up in the wrong region. To correct this, from the Region dropdown in the console, choose the region you provisioned the workshop in. Select Oregon (us-west-2) if you provisioned the workshow per the \u0026ldquo;Start the workshop at an AWS event\u0026rdquo; instructions.\n Once you are on the detail page for the specific CodePipeline, you can see the status along with links to the change and build details.\nIf you click on the \u0026ldquo;details\u0026rdquo; link in the build/deploy stage, you can see the output from the CodeBuild process.\n To review the status of the deployment, you can run:\nkubectl describe deployment hello-k8s  For the status of the service, run the following command:\nkubectl describe service hello-k8s  Challenge: How can we view our exposed service?\nHINT: Which kubectl command will get you the Elastic Load Balancer (ELB) endpoint for this app?\n  Expand here to see the solution   Once the service is built and delivered, we can run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser. If the message is not updated immediately, give Kubernetes some time to deploy the change.\n kubectl get services hello-k8s -o wide  This service was configured with a LoadBalancer so, an AWS Elastic Load Balancer is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n   "
},
{
	"uri": "/monitoring/deploy-grafana/",
	"title": "Deploy Grafana",
	"tags": [],
	"description": "",
	"content": " Download Grafana and update configuration curl -o grafana-values.yaml https://raw.githubusercontent.com/helm/charts/master/stable/grafana/values.yaml  You will make three edits to grafana-values.yaml. Search for storageClassName, uncomment and change the value to \u0026ldquo;prometheus\u0026rdquo;. Search for adminPassword, uncomment and change the password to \u0026ldquo;EKS!sAWSome\u0026rdquo; or something similar. Make a note of this password as you will need it for logging into grafana dashboard later\nThe third edit you will do is for adding Prometheus as a datasource. Search for datasources.yaml and uncomment entire block, update prometheus to the endpoint referred earlier by helm response. The configuration will look similar to below\ndatasources: datasources.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://prometheus-server.prometheus.svc.cluster.local access: proxy isDefault: true  Now let\u0026rsquo;s expose Grafana dashboard using AWS ELB service. Search for service:, and update the value of type: ClusterIP to type: LoadBalancer\n  Expand here to see the complete yaml   rbac: create: true pspEnabled: true serviceAccount: create: true name: replicas: 1 deploymentStrategy: RollingUpdate readinessProbe: httpGet: path: /api/health port: 3000 livenessProbe: httpGet: path: /api/health port: 3000 initialDelaySeconds: 60 timeoutSeconds: 30 failureThreshold: 10 image: repository: grafana/grafana tag: 5.3.1 pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistrKeySecretName securityContext: runAsUser: 472 fsGroup: 472 downloadDashboardsImage: repository: appropriate/curl tag: latest pullPolicy: IfNotPresent ## Pod Annotations # podAnnotations: {} ## Deployment annotations # annotations: {} ## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service). ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it. ## ref: http://kubernetes.io/docs/user-guide/services/ ## service: type: LoadBalancer port: 80 annotations: {} labels: {} ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \u0026quot;true\u0026quot; labels: {} path: / hosts: - chart-example.local tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi ## Node labels for pod assignment ## ref: https://kubernetes.io/docs/user-guide/node-selection/ # nodeSelector: {} ## Tolerations for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ ## tolerations: [] ## Affinity for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity ## affinity: {} ## Enable persistence using Persistent Volume Claims ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## persistence: enabled: false storageClassName: prometheus # accessModes: # - ReadWriteOnce # size: 10Gi # annotations: {} # subPath: \u0026quot;\u0026quot; # existingClaim: adminUser: admin adminPassword: EKS!sAWSome ## Use an alternate scheduler, e.g. \u0026quot;stork\u0026quot;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: ## Extra environment variables that will be pass onto deployment pods env: {} ## The name of a secret in the same kubernetes namespace which contain values to be added to the environment ## This can be useful for auth tokens, etc envFromSecret: \u0026quot;\u0026quot; ## Additional grafana server secret mounts # Defines additional mounts with secrets. Secrets must be manually created in the namespace. extraSecretMounts: [] # - name: secret-files # mountPath: /etc/secrets # secretName: grafana-secret-files # readOnly: true ## Pass the plugins you want installed as a list. ## plugins: [] # - digrich-bubblechart-panel # - grafana-clock-panel ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://prometheus-server.prometheus.svc.cluster.local access: proxy isDefault: true ## Configure grafana dashboard providers ## ref: http://docs.grafana.org/administration/provisioning/#dashboards ## ## `path` must be /var/lib/grafana/dashboards/\u0026lt;provider_name\u0026gt; ## dashboardProviders: {} # dashboardproviders.yaml: # apiVersion: 1 # providers: # - name: 'default' # orgId: 1 # folder: '' # type: file # disableDeletion: false # editable: true # options: # path: /var/lib/grafana/dashboards/default ## Configure grafana dashboard to import ## NOTE: To use dashboards you must also enable/configure dashboardProviders ## ref: https://grafana.com/dashboards ## ## dashboards per provider, use provider name as key. ## dashboards: {} # default: # some-dashboard: # json: | # $RAW_JSON # prometheus-stats: # gnetId: 2 # revision: 2 # datasource: Prometheus # local-dashboard: # url: https://example.com/repository/test.json ## Reference to external ConfigMap per provider. Use provider name as key and ConfiMap name as value. ## A provider dashboards must be defined either by external ConfigMaps or in values.yaml, not in both. ## ConfigMap data example: ## ## data: ## example-dashboard.json: | ## RAW_JSON ## dashboardsConfigMaps: {} # default: \u0026quot;\u0026quot; ## Grafana's primary configuration ## NOTE: values in map will be converted to ini format ## ref: http://docs.grafana.org/installation/configuration/ ## grafana.ini: paths: data: /var/lib/grafana/data logs: /var/log/grafana plugins: /var/lib/grafana/plugins provisioning: /etc/grafana/provisioning analytics: check_for_updates: true log: mode: console grafana_net: url: https://grafana.net ## LDAP Authentication can be enabled with the following values on grafana.ini ## NOTE: Grafana will fail to start if the value for ldap.toml is invalid # auth.ldap: # enabled: true # allow_sign_up: true # config_file: /etc/grafana/ldap.toml ## Grafana's LDAP configuration ## Templated by the template in _helpers.tpl ## NOTE: To enable the grafana.ini must be configured with auth.ldap.enabled ## ref: http://docs.grafana.org/installation/configuration/#auth-ldap ## ref: http://docs.grafana.org/installation/ldap/#configuration ldap: # `existingSecret` is a reference to an existing secret containing the ldap configuration # for Grafana in a key `ldap-toml`. existingSecret: \u0026quot;\u0026quot; # `config` is the content of `ldap.toml` that will be stored in the created secret config: \u0026quot;\u0026quot; # config: |- # verbose_logging = true # [[servers]] # host = \u0026quot;my-ldap-server\u0026quot; # port = 636 # use_ssl = true # start_tls = false # ssl_skip_verify = false # bind_dn = \u0026quot;uid=%s,ou=users,dc=myorg,dc=com\u0026quot; ## Grafana's SMTP configuration ## NOTE: To enable, grafana.ini must be configured with smtp.enabled ## ref: http://docs.grafana.org/installation/configuration/#smtp smtp: # `existingSecret` is a reference to an existing secret containing the smtp configuration # for Grafana in keys `user` and `password`. existingSecret: \u0026quot;\u0026quot; ## Sidecars that collect the configmaps with specified label and stores the included files them into the respective folders ## Requires at least Grafana 5 to work and can't be used together with parameters dashboardProviders, datasources and dashboards sidecar: image: kiwigrid/k8s-sidecar:0.0.3 imagePullPolicy: IfNotPresent resources: # limits: # cpu: 100m # memory: 100Mi # requests: # cpu: 50m # memory: 50Mi dashboards: enabled: false # label that the configmaps with dashboards are marked with label: grafana_dashboard # folder in the pod that should hold the collected dashboards folder: /tmp/dashboards datasources: enabled: false # label that the configmaps with datasources are marked with label: grafana_datasource    Deploy grafana helm install -f grafana-values.yaml stable/grafana --name grafana --namespace grafana  Run the command to check if Grafana is running properly\nkubectl get all -n grafana  You should see similar results. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/grafana-b9697f8b5-t9w4j 1/1 Running 0 2m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/grafana LoadBalancer 10.100.49.172 abe57f85de73111e899cf0289f6dc4a4-1343235144.us-west-2.elb.amazonaws.com 80:31570/TCP 3m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/grafana 1 1 1 1 2m NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-b9697f8b5 1 1 1 2m  You can get Grafana ELB URL using this command. Copy \u0026amp; Paste the value into browser to access Grafana web UI\nexport ELB=$(kubectl get svc -n grafana grafana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') echo \u0026quot;http://$ELB\u0026quot;  "
},
{
	"uri": "/statefulset/services/",
	"title": "Criar serviços",
	"tags": [],
	"description": "",
	"content": " Introdução Serviço Kubernetes define um conjunto lógico de pods e uma política pela qual acessá-los. O serviço pode ser exposto de diferentes formas, especificando um tipo no serviceSpec. StatefulSet atualmente requer um Headless Service para controlar o domínio de seus Pods, alcançar diretamente cada Pod com entradas de DNS estáveis. Especificando \u0026ldquo;None\u0026rdquo; para o clusterIP, você pode criar o serviço Headless.\nCriar serviços Copie/Cole os seguintes comandos no seu terminal Cloud9.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/services.files/mysql-services.yml  Verifique a configuração do mysql-services.yml, com o seguinte comando.\ncat ~/environment/templates/mysql-services.yml  Você pode ver que o serviço mysql é para resolução de DNS, de modo que quando os pods são colocados pelo controlador StatefulSet, pods podem ser resolvidos usando pod-name.mysql. mysql-read é um serviço de cliente que faz o balanceamento de carga para todos slaves.\n# Serviço Headless de DNS estáveis ​​de membros do StatefulSet. apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- # Serviço de cliente para conectar-se a qualquer instância do MySQL para leituras. # Para gravações, você deve se conectar ao master: mysql-0.mysql. apiVersion: v1 kind: Service metadata: name: mysql-read labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql  Criar serviço mysql e mysql-read com o seguinte comando\nkubectl create -f ~/environment/templates/mysql-services.yml    Related files   mysql-services.yml  (0 ko)    "
},
{
	"uri": "/codepipeline/change/",
	"title": "Trigger New Release",
	"tags": [],
	"description": "",
	"content": " Update Our Application So far we have walked through setting up CI/CD for EKS using AWS CodePipeline and now we are going to make a change to the GitHub repository so that we can see a new release built and delivered.\nOpen GitHub and select the forked repository with the name eks-workshop-sample-api-service-go.\nClick on main.go file and then click on the edit button, which looks like a pencil.\nChange the text where it says \u0026ldquo;Hello World\u0026rdquo;, add a commit message and then click the \u0026ldquo;Commit changes\u0026rdquo; button.\nYou should leave the master branch selected.\nThe main.go application needs to be compiled, so please ensure that you don\u0026rsquo;t accidentally break the build :)\n After you modify and commit your change in GitHub, in approximately one minute you will see a new build triggered in the AWS Management Console Confirm the Change If you still have the ELB URL open in your browser, refresh to confirm the update. If you need to retrieve the URL again, use kubectl get services hello-k8s -o wide\n"
},
{
	"uri": "/codepipeline/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Parabéns pela conclusão do CI/CD com o módulo CodePipeline.\nThis module is not used in subsequent steps, so you can remove the resources now, or at the end of the workshop.\nFirst we need to delete the Kubernetes deployment and service:\nkubectl delete deployments hello-k8s kubectl delete services hello-k8s  Next, we are going to delete the CloudFormation stack created. Open CloudFormation the AWS Management Console.\nCheck the box next to the eksws-codepipeline stack, select the Actions dropdown menu and then click Delete stack:\nNow we are going to delete the ECR repository:\nEmpty and then delete the S3 bucket used by CodeBuild for build artifacts (bucket name starts with eksws-codepipeline). First, select the bucket, then empty the bucket and finally delete the bucket:\nFinally, we are going to delete the IAM role created for CodeBuild to permit changes to the EKS cluster:\naws iam delete-role-policy --role-name EksWorkshopCodeBuildKubectlRole --policy-name eks-describe aws iam delete-role --role-name EksWorkshopCodeBuildKubectlRole  "
},
{
	"uri": "/batch/jobs/",
	"title": "Kubernetes Jobs",
	"tags": [],
	"description": "",
	"content": " Kubernetes Jobs Um job cria um ou mais pods e garante que um número especificado deles termine com sucesso. À medida que os pods são concluídos com êxito, o trabalho rastreia as conclusões bem-sucedidas. Quando um número especificado de conclusões bem-sucedidas é atingido, o trabalho em si é concluído. Excluir um Job limpará os pods criados.\nSalve o manifesto abaixo como \u0026lsquo;job-whalesay.yaml\u0026rsquo; usando seu editor favorito.\napiVersion: batch/v1 kind: Job metadata: name: whalesay spec: template: spec: containers: - name: whalesay image: docker/whalesay command: [\u0026quot;cowsay\u0026quot;, \u0026quot;This is a Kubernetes Job!\u0026quot;] restartPolicy: Never backoffLimit: 4  Execute um exemplo de job do Kubernetes usando a imagem whalesay .\nkubectl apply -f job-whalesay.yaml  Wait until the job has completed successfully.\nkubectl get job/whalesay  NAME DESIRED SUCCESSFUL AGE whalesay 1 1 2m  Confirm the output.\nkubectl logs -l job-name=whalesay  ___________________________ \u0026lt; This is a Kubernetes Job! \u0026gt; --------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  "
},
{
	"uri": "/servicemesh/download/",
	"title": "Baixe e instale o Istio CLI",
	"tags": [],
	"description": "",
	"content": "Antes de podermos começar a configurar o Istio, precisamos primeiro instalar as ferramentas de linha de comando com as quais você irá interagir. Para fazer isso, execute o seguinte.\ncd ~/environment curl -L https://git.io/getLatestIstio | sh - // versão pode ser diferente como istio é atualizado cd istio-1.0.3 sudo mv -v bin/istioctl /usr/local/bin/  "
},
{
	"uri": "/monitoring/dashboards/",
	"title": "Dashboards",
	"tags": [],
	"description": "",
	"content": " Criar Dashboards Faça o login no Dashboards do Grafana usando as credenciais fornecidas durante a configuração Você vai notar que \u0026lsquo;Install Grafana\u0026rsquo; \u0026amp; \u0026lsquo;crie sua primeira fonte de dados\u0026rsquo; já estão concluídas. Vamos importar o Dashboards criado pela comunidade para este tutorial\nClique \u0026lsquo;+\u0026lsquo;no botão no painel esquerdo e selecione \u0026lsquo;Import\u0026rsquo;\nEnter 3131 dashboard id under Grafana.com Dashboard \u0026amp; click \u0026lsquo;Load\u0026rsquo;.\nLeave the defaults, select \u0026lsquo;Prometheus\u0026rsquo; as the endpoint under prometheus data sources drop down, click \u0026lsquo;Import\u0026rsquo;.\nThis will show monitoring dashboard for all cluster nodes\nFor creating dashboard to monitor all pods, repeat same process as above and enter 3146 for dashboard id\n"
},
{
	"uri": "/introduction/basics/",
	"title": "Kubernetes (k8s) Noções básicas",
	"tags": [],
	"description": "",
	"content": "Nesta seção, abordaremos os seguintes tópicos:\n O que é o Kubernetes??   Nodes do Kubernetes   Visão Geral dos Objetos K8s   Detalhe de objetos do K8s(1/2)   Detalhe de objetos do K8s (2/2)   "
},
{
	"uri": "/deploy/deploycrystal/",
	"title": "Implantar a API do Crystal Backend",
	"tags": [],
	"description": "",
	"content": "Vamos trazer a API do Crystal Backend!\nCopie/cole os seguintes comandos no seu workspace Cloud9 :\ncd ~/environment/ecsdemo-crystal kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  Podemos acompanhar o progresso observando o status do deployment:\nkubectl get deployment ecsdemo-crystal  "
},
{
	"uri": "/cleanup/undeploy/",
	"title": "Undeploy the applications",
	"tags": [],
	"description": "",
	"content": "Para excluir os recursos criados pelos aplicativos, devemos excluir as implantações do aplicativo e o dashboard do kubernetes:\nremover Implantação das aplicações:\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml  "
},
{
	"uri": "/tabs-example/tabs/eksctl/",
	"title": "Launch EKS",
	"tags": [],
	"description": "",
	"content": "To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n "
},
{
	"uri": "/terraform/module/",
	"title": "Obter o módulo Terraform",
	"tags": [],
	"description": "",
	"content": "Agora vamos clonar o módulo terraform da comunidade para o EKS:\ncd ${HOME}/environment/ git clone https://github.com/WesleyCharlesBlake/terraform-aws-eks.git cd terraform-aws-eks  "
},
{
	"uri": "/spotworkers/deployhandler/",
	"title": "Deploy The Spot Interrupt Handler",
	"tags": [],
	"description": "",
	"content": " In this section, we will prepare our cluster to handle Spot interruptions.\nIf the available On-Demand capacity of a particular instance type is depleted, the Spot Instance is sent an interruption notice two minutes ahead to gracefully wrap up things. We will deploy a pod on each spot instance to detect and redeploy applications elsewhere in the cluster\nThe first thing that we need to do is deploy the Spot Interrupt Handler on each Spot Instance. This will monitor the EC2 metadata service on the instance for a interruption notice.\nThe workflow can be summarized as:\n Identify that a Spot Instance is being reclaimed. Use the 2-minute notification window to gracefully prepare the node for termination. Taint the node and cordon it off to prevent new pods from being placed. Drain connections on the running pods. Replace the pods on remaining nodes to maintain the desired capacity.  We have provided an example K8s DaemonSet manifest. A DaemonSet runs one pod per node.\nmkdir ~/environment/spot cd ~/environment/spot wget https://eksworkshop.com/spot/managespot/deployhandler.files/spot-interrupt-handler-example.yml  As written, the manifest will deploy pods to all nodes including On-Demand, which is a waste of resources. We want to edit our DaemonSet to only be deployed on Spot Instances. Let\u0026rsquo;s use the labels to identify the right nodes.\nUse a nodeSelector to constrain our deployment to spot instances. View this link for more details.\nChallenge Configure our Spot Handler to use nodeSelector   Expand here to see the solution   Place this at the end of the DaemonSet manifest under Spec.Template.Spec.nodeSelector\nnodeSelector: lifecycle: Ec2Spot   \nDeploy the DaemonSet\nkubectl apply -f ~/environment/spot/spot-interrupt-handler-example.yml  If you receive an error deploying the DaemonSet, there is likely a small error in the YAML file. We have provided a solution file at the bottom of this page that you can use to compare.\n View the pods. There should be one for each spot node.\nkubectl get daemonsets    Related files   spot-interrupt-handler-example.yml  (1 ko)   spot-interrupt-handler-solution.yml  (1 ko)    "
},
{
	"uri": "/statefulset/statefulset/",
	"title": "Create StatefulSet",
	"tags": [],
	"description": "",
	"content": " Introduction StatefulSet consists of serviceName, replicas, template and volumeClaimTemplates. serviceName is \u0026ldquo;mysql\u0026rdquo;, headless service we created in previous section, replicas is 3, the desired number of pod, template is the configuration of pod, volumeClaimTemplates is to claim volume for pod based on storageClassName, gp2 that we created in \u0026ldquo;Define Storageclass\u0026rdquo; section. Percona Xtrabackup is in template to clone source MySQL server to its slaves.\nCreate StatefulSet Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/statefulset.files/mysql-statefulset.yml  Create statefulset \u0026ldquo;mysql\u0026rdquo; by following command.\nkubectl create -f ~/environment/templates/mysql-statefulset.yml  Watch StatefulSet Watch the status of statefulset.\nkubectl get -w statefulset  It will take few minutes for pods to initialize and have statefulset created. DESIRED is the replicas number you define at StatefulSet.\nNAME DESIRED CURRENT AGE mysql 3 1 8s mysql 3 2 59s mysql 3 3 2m mysql 3 3 3m  Open another Cloud9 Terminal and watch the progress of pods creation using the following command.\nkubectl get pods -l app=mysql --watch  You can see ordered, graceful deployment with a stable, unique name for each pod.\nNAME READY STATUS RESTARTS AGE mysql-0 0/2 Init:0/2 0 30s mysql-0 0/2 Init:1/2 0 35s mysql-0 0/2 PodInitializing 0 47s mysql-0 1/2 Running 0 48s mysql-0 2/2 Running 0 59s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Init:0/2 0 0s mysql-1 0/2 Init:1/2 0 35s mysql-1 0/2 Init:1/2 0 45s mysql-1 0/2 PodInitializing 0 54s mysql-1 1/2 Running 0 55s mysql-1 2/2 Running 0 1m mysql-2 0/2 Pending 0 \u0026lt;invalid\u0026gt; mysql-2 0/2 Pending 0 \u0026lt;invalid\u0026gt; mysql-2 0/2 Pending 0 0s mysql-2 0/2 Init:0/2 0 0s mysql-2 0/2 Init:1/2 0 32s mysql-2 0/2 Init:1/2 0 43s mysql-2 0/2 PodInitializing 0 50s mysql-2 1/2 Running 0 52s mysql-2 2/2 Running 0 56s  Press Ctrl+C to stop watching.\nCheck the dynamically created PVC by following command.\nkubectl get pvc -l app=mysql  You can see data-mysql-0,1,2 are created by STORAGECLASS mysql-gp2.\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-mysql-0 Bound pvc-83e9dfeb-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-1 Bound pvc-977e7806-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-2 Bound pvc-b3009b02-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d  (Optional) Check 10Gi 3 EBS volumes are created across availability zones at your AWS console.   Related files   mysql-statefulset.yml  (5 ko)    "
},
{
	"uri": "/helm_root/helm_micro/customize/",
	"title": "Personalizar Padrões",
	"tags": [],
	"description": "",
	"content": " Se você olhar no diretório eksdemo recém-criado, verá vários arquivos e diretórios. Especificamente, dentro do diretório /templates, você verá:\n NOTES.txt: O \u0026lsquo;texto de ajuda\u0026rsquo; do seu chart. Isso será exibido para seus usuários quando eles executarem a instalação do helm. deployment.yaml: Um manifesto básico para criar uma implantação do Kubernetes service.yaml: Um manifesto básico para criar um terminal de serviço para seu deploy _helpers.tpl: Um lugar para colocar template helpers que você pode reutilizar em todo o chart  Na verdade, vamos criar nossos próprios arquivos, então vamos excluir esses arquivos clichês\nrm -rf ~/environment/eksdemo/templates/ rm ~/environment/eksdemo/Chart.yaml rm ~/environment/eksdemo/values.yaml  Crie um novo arquivo Chart.yaml que irá descrever o Chart\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/Chart.yaml apiVersion: v1 appVersion: \u0026quot;1.0\u0026quot; description: A Helm chart for EKS Workshop Microservices application name: eksdemo version: 0.1.0 EoF  Em seguida, copiaremos os arquivos de manifesto de cada um dos nossos microservices no diretório de templates servicename.yaml\n#criar subpastas para cada tipo de template mkdir -p ~/environment/eksdemo/templates/deployment mkdir -p ~/environment/eksdemo/templates/service # Copiar e renomear manifestos do frontend cp ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/frontend.yaml cp ~/environment/ecsdemo-frontend/kubernetes/service.yaml ~/environment/eksdemo/templates/service/frontend.yaml # Copiar e renomear manifestos de cristal cp ~/environment/ecsdemo-crystal/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/crystal.yaml cp ~/environment/ecsdemo-crystal/kubernetes/service.yaml ~/environment/eksdemo/templates/service/crystal.yaml # Copie e renomeie os manifestos de nodejs cp ~/environment/ecsdemo-nodejs/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/nodejs.yaml cp ~/environment/ecsdemo-nodejs/kubernetes/service.yaml ~/environment/eksdemo/templates/service/nodejs.yaml  Todos os arquivos no diretório de templates são enviados pelo mecanismo de template. Atualmente, esses arquivos YAML simples são enviados ao Kubernetes no estado em que se encontram.\nSubstituir valores codificados por diretivas de template Vamos substituir alguns dos valores por \u0026lsquo;diretivas de template\u0026rsquo; para permitir mais personalização removendo os valores codificados.\nAbra ~/environment/eksdemo/templates/deployment/frontend.yaml no seu editor Cloud9.\nAs etapas a seguir devem ser concluídas separadamente frontend.yaml, crystal.yaml, e nodejs.yaml.\n Under spec, find replicas: 1 and replace with the following:\nreplicas: {{ .Values.replica }}  Debaixo spec.template.spec.containers.image, substitua a imagem pelo valor de template correto da tabela abaixo:\n   Filename Value     frontend.yaml image: {{ .Values.frontend.image }}:{{ .Values.version }}   crystal.yaml image: {{ .Values.crystal.image }}:{{ .Values.version }}   nodejs.yaml image: {{ .Values.nodejs.image }}:{{ .Values.version }}    Crie um arquivo values.yaml com nossos padrões de template Este arquivo irá preencher nossas diretivas de template com os valores padrão.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/values.yaml # Valores padrão para o eksdemo. # Este é um arquivo formatado em YAML. # Declare variáveis ​​a serem passadas para seus templates. # Valores de todo o realease replica: 3 version: 'latest' # Valores Específicos de Serviço nodejs: image: brentley/ecsdemo-nodejs crystal: image: brentley/ecsdemo-crystal frontend: image: brentley/ecsdemo-frontend EoF  "
},
{
	"uri": "/logging/setup_es/",
	"title": "Provisionar um cluster do Elasticsearch",
	"tags": [],
	"description": "",
	"content": "Este exemplo cria um cluster de duas instâncias do Amazon Elasticsearch chamado kubernetes-logs. Esse cluster é criado na mesma região que o cluster do Kubernetes e o grupo de log do CloudWatch.\nObserve que esse cluster tem uma política de acesso aberto que precisa ser bloqueada em ambientes de produção.\n aws es create-elasticsearch-domain \\ --domain-name kubernetes-logs \\ --elasticsearch-version 6.3 \\ --elasticsearch-cluster-config \\ InstanceType=m4.large.elasticsearch,InstanceCount=2 \\ --ebs-options EBSEnabled=true,VolumeType=standard,VolumeSize=100 \\ --access-policies '{\u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;,\u0026quot;Statement\u0026quot;:[{\u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;,\u0026quot;Principal\u0026quot;:{\u0026quot;AWS\u0026quot;:[\u0026quot;*\u0026quot;]},\u0026quot;Action\u0026quot;:[\u0026quot;es:*\u0026quot;],\u0026quot;Resource\u0026quot;:\u0026quot;*\u0026quot;}]}'  Demora um pouco para o cluster ser criado e chegar a um estado ativo. O AWS Console deve mostrar o seguinte status quando o cluster estiver pronto.\nVocê também pode verificar isso via AWS CLI:\naws es describe-elasticsearch-domain --domain-name kubernetes-logs --query 'DomainStatus.Processing'  Se o valor de saída for falso, significa que o domínio foi processado e agora está disponível para uso.\nSinta-se à vontade para seguir para a próxima seção por enquanto.\n"
},
{
	"uri": "/scaling/test_hpa/",
	"title": "Scale an Application with HPA",
	"tags": [],
	"description": "",
	"content": " Implantar um aplicativo de exemplo Vamos implantar um aplicativo e expor como um serviço na porta TCP 80. O aplicativo é uma imagem customizada baseada na imagem php-apache. A página index.php realiza cálculos para gerar carga da CPU. Mais informações podem ser encontradas Aqui\nkubectl run php-apache --image=k8s.gcr.io/hpa-example --requests=cpu=200m --expose --port=80  Crie um recurso HPA Este HPA aumenta quando a CPU excede 50% do recurso de contêiner alocado.\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10  Veja o HPA usando o kubectl. Você provavelmente verá \u0026lt;unknown\u0026gt;/50% por 1-2 minutos e então você deve ser capaz de ver 0%/50%\nkubectl get hpa  Gerar carga para acionar o dimensionamento Abra um novo terminal no Cloud9 Environment e execute o seguinte comando para colocar em um shell em um novo contêiner\nkubectl run -i --tty load-generator --image=busybox /bin/sh  Execute um loop while para continuar recebendo http:///php-apache\nwhile true; do wget -q -O - http://php-apache; done  Na guia anterior, observe o HPA com o seguinte comando\nkubectl get hpa -w  Você verá a HPA dimensionar os pods de 1 até o máximo configurado (10) até que a média da CPU esteja abaixo do nosso objetivo (50%)\nAgora você pode interromper o teste de carga (Ctrl C) que estava sendo executado no outro terminal. Você notará que o HPA levará lentamente a contagem da réplica ao número mínimo com base em sua configuração. Você também deve sair do aplicativo de teste de carga pressionando Ctrl D\n"
},
{
	"uri": "/prerequisites/aws_event/portal/",
	"title": "AWS Workshop Portal",
	"tags": [],
	"description": "",
	"content": " Faça o login no AWS Workshop Portal Este workshop cria uma conta da AWS e um ambiente Cloud9. Você precisará do Participant Hash fornecido na entrada e do seu endereço de e-mail para acompanhar sua sessão exclusiva.\nConecte-se ao portal clicando no botão ou navegando para https://portal.awsworkshop.io/.\nConnect to Portal  \nInsira seu Participant Hash e seu endereço de e-mail e clique em Log In.\nDepois de ter feito o login, primeiro faça o login no console da AWS clicando no botão . Depois de ter feito o login com sucesso no AWS Console, você pode abrir o IDE do Cloud9 clicando no botão .\nO workshop adicionou uma função do IAM para realizar todas as etapas do workshop no Cloud9 Environment. Você não precisa adicionar uma função à instância que liga o ambiente Cloud9.\n Depois de ter concluído o passo acima, você pode ir direto para Create a SSH key section\n"
},
{
	"uri": "/prerequisites/sshkey/",
	"title": "Crie uma chave SSH",
	"tags": [],
	"description": "",
	"content": "A partir daqui, quando você ver o comando a ser inserido, como abaixo, você inserirá esses comandos na IDE do Cloud9 . Você pode usar o recurso Copiar para área de transferência (canto superior direito) para simplesmente copiar e colar na Cloud9. Para colar, você pode usar Ctrl V para Windows ou Command V para Mac.\n Por favor, execute este comando para gerar a chave SSH na Cloud9. Essa chave será usada nas instâncias dos worker node para permitir o acesso ssh, se necessário.\nssh-keygen  Pressione enter 3 vezes para escolher as opções padrão\n Carregue a chave pública na sua região EC2:\naws ec2 import-key-pair --key-name \u0026quot;eksworkshop\u0026quot; --public-key-material file://~/.ssh/id_rsa.pub  "
},
{
	"uri": "/prerequisites/k8stools/",
	"title": "Instalar ferramentas do Kubernetes",
	"tags": [],
	"description": "",
	"content": " Os clusters do Amazon EKS requerem binários kubectl e kubelet e o binário aws-iam-authenticator para permitir a autenticação do IAM para seu cluster do Kubernetes.\nNeste workshop nós lhe daremos os comandos para baixar os binários do Linux. Se você estiver executando o Mac OSX / Windows, por favor consulte os documentos oficiais do EKS para os links de download.\n Crie o diretório ~/.kube padrão para armazenar a configuração do kubectl mkdir -p ~/.kube  Instalar o kubectl sudo curl --silent --location -o /usr/local/bin/kubectl \u0026quot;https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/kubectl\u0026quot; sudo chmod +x /usr/local/bin/kubectl  Instale o AWS IAM Authenticator go get -u -v github.com/kubernetes-sigs/aws-iam-authenticator/cmd/aws-iam-authenticator sudo mv ~/go/bin/aws-iam-authenticator /usr/local/bin/aws-iam-authenticator  Verifique os binários kubectl version --short --client aws-iam-authenticator help  Install JQ sudo yum -y install jq  "
},
{
	"uri": "/prerequisites/clone/",
	"title": "Clone os Repos de Serviço",
	"tags": [],
	"description": "",
	"content": "cd ~/environment git clone https://github.com/brentley/ecsdemo-frontend.git git clone https://github.com/brentley/ecsdemo-nodejs.git git clone https://github.com/brentley/ecsdemo-crystal.git  "
},
{
	"uri": "/monitoring/cleanup/",
	"title": "Limpar",
	"tags": [],
	"description": "",
	"content": " Excluir Prometheus e grafana helm delete prometheus helm del --purge prometheus helm delete grafana helm del --purge grafana  "
},
{
	"uri": "/deploy/servicetype/",
	"title": "Vamos checar tipos de serviço",
	"tags": [],
	"description": "",
	"content": "Antes de abrirmos o serviço frontend, vamos dar uma olhada nos tipos de serviço que estamos usando: Isto é o kubernetes/service.yaml para nosso serviço de frontend:\napiVersion: v1 kind: Service metadata: name: ecsdemo-frontend spec: selector: app: ecsdemo-frontend type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 3000  Observe type: LoadBalancer Isso configurará um ELB para manipular o tráfego de entrada para este serviço.\nCompare isso com kubernetes/service.yaml para um dos nossos serviços de back-end:\napiVersion: v1 kind: Service metadata: name: ecsdemo-nodejs spec: selector: app: ecsdemo-nodejs ports: - protocol: TCP port: 80 targetPort: 3000  Observe que não há nenhum tipo de serviço específico descrito. Quando nós verificamosa documentação do kubernetes descobrimos que o tipo padrão é ClusterIP. Isso expõe o serviço em um IP interno do cluster. A escolha desse valor torna o serviço acessível somente dentro do cluster.\n"
},
{
	"uri": "/statefulset/testmysql/",
	"title": "Teste o MySQL",
	"tags": [],
	"description": "",
	"content": "Você pode usar mysql-client para enviar alguns dados para o master, mysql-0.mysql executando o comando.\nkubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\\ mysql -h mysql-0.mysql \u0026lt;\u0026lt;EOF CREATE DATABASE test; CREATE TABLE test.messages (message VARCHAR(250)); INSERT INTO test.messages VALUES ('hello, from mysql-client'); EOF  Execute o seguinte para testar slaves (mysql-read) recebeu o data.\nkubectl run mysql-client --image=mysql:5.7 -it --rm --restart=Never --\\ mysql -h mysql-read -e \u0026quot;SELECT * FROM test.messages\u0026quot;  A saída deve ficar assim.\n+--------------------------+ | message | +--------------------------+ | hello, from mysql-client | +--------------------------+  Para testar o balanceamento de carga entre slaves, execute o seguinte comando.\nkubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\ bash -ic \u0026quot;while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done\u0026quot;  Cada instância do MySQL é atribuída a um identificador único e pode ser recuperada usando @@server_id. Ele imprimirá o ID do servidor que atende a solicitação e o timestamp.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 102 | 2018-11-14 12:44:57 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 12:44:58 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 12:44:59 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 12:45:00 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 12:45:01 | +-------------+---------------------+  Deixe isso aberto em uma janela separada enquanto você testa a falha na próxima seção.\n"
},
{
	"uri": "/prerequisites/self_paced/iamrole/",
	"title": "Criar uma role do IAM para sua área de trabalho",
	"tags": [],
	"description": "",
	"content": " Siga este link direto para criar uma role do IAM com acesso de administrador. Confirme que AWS service e EC2 estão selecionados e, em seguida, clique em Next para visualizar as permissões. Confirme se AdministratorAccess está marcado e clique em Next para revisar. Digite eksworkshop-admin para o nome e selecione Create Role   "
},
{
	"uri": "/healthchecks/cleanup/",
	"title": "Limpar",
	"tags": [],
	"description": "",
	"content": "Nosso exemplo do Liveness Probe usou a solicitação HTTP e o Readiness Probe executou um comando para verificar a integridade de um pod. O mesmo pode ser feito usando uma solicitação TCP, conforme descrito em documentation.\n kubectl delete -f ~/environment/healthchecks/liveness-app.yaml kubectl delete -f ~/environment/healthchecks/readiness-deployment.yaml  "
},
{
	"uri": "/terraform/",
	"title": "Provisionar  usando o Terraform",
	"tags": [],
	"description": "",
	"content": " Como usar o Terraform Também temos algumas ferramentas de parceiros muito poderosas que nos permitem automatizar grande parte da experiência de criação de um cluster EKS, simplificando o processo.\nNeste módulo, vamos destacar um módulo criado pela comunidade terraform e usá-la para lançar e configurar nosso cluster EKS e nós.\n"
},
{
	"uri": "/prerequisites/self_paced/ec2instance/",
	"title": "Anexe a role do IAM ao seu Workspace",
	"tags": [],
	"description": "",
	"content": " Siga este link direto para encontrar sua instância do Cloud9 EC2 Selecione a instância e escolha Actions / Instance Settings / Attach/Replace IAM Role  Choose eksworkshop-admin from the IAM Role drop down, and select Apply   "
},
{
	"uri": "/deploy/servicerole/",
	"title": "Assegure-se de que a função de serviço do ELB exista",
	"tags": [],
	"description": "",
	"content": "Nas contas da AWS que nunca criaram um balanceador de carga antes, é possível que a função de serviço para o ELB ainda não exista.\nPodemos verificar o papel e criá-lo se estiver faltando.\nCopie/cole os seguintes comandos no seu workspace Cloud9 :\naws iam get-role --role-name \u0026quot;AWSServiceRoleForElasticLoadBalancing\u0026quot; || aws iam create-service-linked-role --aws-service-name \u0026quot;elasticloadbalancing.amazonaws.com\u0026quot;  "
},
{
	"uri": "/batch/install/",
	"title": "Install Argo CLI",
	"tags": [],
	"description": "",
	"content": " Install Argo CLI Before we can get started configuring argo we\u0026rsquo;ll need to first install the command line tools that you will interact with. To do this run the following.\nsudo curl -sSL -o /usr/local/bin/argo https://github.com/argoproj/argo/releases/download/v2.2.1/argo-linux-amd64 sudo chmod +x /usr/local/bin/argo  "
},
{
	"uri": "/servicemesh/install/",
	"title": "Instale o Istio",
	"tags": [],
	"description": "",
	"content": " Instalar o CRD do Istio O Custom Resource Definition, também conhecido como CRD, é um recurso da API que permite definir recursos personalizados.\nkubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml  Instale o Istio Helm é necessário para os exemplos a seguir. Se você ainda não instalou o Helm, por favor, primeiro referenciar o capítulo Helm Antes de prosseguir.\nkubectl create -f install/kubernetes/helm/helm-service-account.yaml helm template install/kubernetes/helm/istio --name istio --namespace istio-system --set global.configValidation=false --set sidecarInjectorWebhook.enabled=false --set grafana.enabled=true --set servicegraph.enabled=true \u0026gt; istio.yaml kubectl create namespace istio-system kubectl apply -f istio.yaml  Veja o progresso de instalação usando:\nkubectl get pod -n istio-system -w  E pressione CTRL-C quando estiver pronto para prosseguir.\nNAME READY STATUS RESTARTS AGE grafana-9cfc9d4c9-csvw7 1/1 Running 0 3m istio-citadel-6d7f9c545b-w7hjs 1/1 Running 0 3m istio-cleanup-secrets-vrkm5 0/1 Completed 0 3m istio-egressgateway-866885bb49-cz6jr 1/1 Running 0 3m istio-galley-6d74549bb9-t8sqb 1/1 Running 0 3m istio-grafana-post-install-4bgxv 0/1 Completed 0 3m istio-ingressgateway-6c6ffb7dc8-dnmqx 1/1 Running 0 3m istio-pilot-685fc95d96-jhfhv 2/2 Running 0 3m istio-policy-688f99c9c4-pb558 2/2 Running 0 3m istio-security-post-install-5dw8n 0/1 Completed 0 3m istio-telemetry-69b794ff59-spkp2 2/2 Running 0 3m prometheus-f556886b8-cxb9n 1/1 Running 0 3m servicegraph-778f94d6f8-tfmp6 1/1 Running 0 3m  "
},
{
	"uri": "/introduction/basics/what_is_k8s/",
	"title": "O que é o Kubernetes??",
	"tags": [],
	"description": "",
	"content": " Com base em mais de uma década de experiência e melhores práticas Utiliza configuração declarativa e automação Desenha um grande ecossistema de ferramentas, serviços, suporte  Mais informações sobre o que é o Kubernetes podem ser encontradas no site oficial do Kubernetes.\n"
},
{
	"uri": "/spotworkers/preferspot/",
	"title": "Deploy an Application on Spot",
	"tags": [],
	"description": "",
	"content": " We are redesigning our Microservice example and want our frontend service to be deployed on Spot Instances when they are available. We will use Node Affinity in our manifest file to configure this.\nConfigure Node Affinity and Tolerations Open the deployment manifest in your Cloud9 editor - ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml\nEdit the spec to configure NodeAffinity to prefer Spot Instances, but not require them. This will allow the pods to be scheduled on On-Demand nodes if no spot instances were available or correctly labelled.\nWe also want to configure a toleration which will allow the pods to \u0026ldquo;tolerate\u0026rdquo; the taint that we configured on our EC2 Spot Instances.\nFor examples of Node Affinity, check this link\nFor examples of Taints and Tolerations, check this link\nChallenge Configure Affinity and Toleration\n  Expand here to see the solution   Add this to your deployment file under spec.template.spec\naffinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: lifecycle operator: In values: - Ec2Spot tolerations: - key: \u0026quot;spotInstance\u0026quot; operator: \u0026quot;Equal\u0026quot; value: \u0026quot;true\u0026quot; effect: \u0026quot;PreferNoSchedule\u0026quot;  We have provided a solution file below that you can use to compare.\n     Related files   deployment-solution.yml  (1 ko)    Redeploy the Frontend on Spot First let\u0026rsquo;s take a look at all pods deployed on Spot instances\nfor n in $(kubectl get nodes -l lifecycle=Ec2Spot --no-headers | cut -d \u0026quot; \u0026quot; -f1); do kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; done  Now we will redeploy our microservices with our edited Frontend Manifest\ncd ~/environment/ecsdemo-frontend kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml  We can again check all pods deployed on Spot Instances and should now see the frontend pods running on Spot instances\nfor n in $(kubectl get nodes -l lifecycle=Ec2Spot --no-headers | cut -d \u0026quot; \u0026quot; -f1); do kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; done  "
},
{
	"uri": "/deploy/deployfrontend/",
	"title": "Implantar o serviço de front-end",
	"tags": [],
	"description": "",
	"content": "Vamos implantar o Frontend Ruby !\nCopie/cole os seguintes comandos no seu workspace Cloud9 :\ncd ~/environment/ecsdemo-frontend kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  Podemos acompanhar o progresso observando o status da implantação:\nkubectl get deployment ecsdemo-frontend  "
},
{
	"uri": "/cleanup/eksctl/",
	"title": "Delete the EKSCTL Cluster",
	"tags": [],
	"description": "",
	"content": "Para excluir os recursos criados para esse cluster EKS, execute os seguintes comandos:\nExclua o cluster:\neksctl delete cluster --name=eksworkshop-eksctl  O grupo de nós terá que concluir o processo de exclusão antes que o cluster EKS possa ser excluído. O processo total levará aproximadamente 15 minutos e pode ser monitorado pelo Console do CloudFormation \n "
},
{
	"uri": "/terraform/launcheks/",
	"title": "Launch EKS Cluster",
	"tags": [],
	"description": "",
	"content": "Começamos inicializando o estado do Terraform:\nterraform init  Agora podemos executar o plan do nosso deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  E se quisermos aplicar esse plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  A aplicação do novo plan do terraform levará aproximadamente 15 minutos\n "
},
{
	"uri": "/dashboard/connect/",
	"title": "Acesse o Dashboard",
	"tags": [],
	"description": "",
	"content": "Agora podemos acessar o Dashboard do Kubernetes\n Em seu ambiente Cloud9, clique Preview / Preview Running Application Vá até the end of the URL e acrescente:  /api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/  Abra uma nova guia Terminal e digite\naws-iam-authenticator token -i eksworkshop-eksctl --token-only  Copie a saída deste comando e, em seguida, click o botão em formato radio ao lado Token em seguida, no campo de texto abaixo, cole a saída do último comando.\nEntão aperte Sign In.\nSe você quiser ver o Dashboard em uma guia completa, clique no botão Pop Out , como abaixo: "
},
{
	"uri": "/scaling/deploy_ca/",
	"title": "Configure Cluster Autoscaler (CA)",
	"tags": [],
	"description": "",
	"content": " Autoscaler do Cluster, A AWS fornece integração com grupos do Auto Scaling. Ele permite que os usuários escolham entre quatro opções diferentes de deployment:\n One Auto Scaling group - This is what we will use Multiple Auto Scaling groups Auto-Discovery Master Node setup  Configure the Cluster Autoscaler (CA) We have provided a manifest file to deploy the CA. Copy the commands below into your Cloud9 Terminal.\nmkdir ~/environment/cluster-autoscaler cd ~/environment/cluster-autoscaler wget https://eksworkshop.com/scaling/deploy_ca.files/cluster_autoscaler.yml  Configure the ASG We will need to provide the name of the Autoscaling Group that we want CA to manipulate. Collect the name of the Auto Scaling Group (ASG) containing your worker nodes. Record the name somewhere. We will use this later in the manifest file.\nYou can find it in the console by following this link.\nCheck the box beside the ASG and click Actions and Edit\nChange the following settings:\n Min: 2 Max: 8  Click Save\nConfigurar o autoescalador do cluster Usando o navegador de arquivos à esquerda, abra cluster_autoscaler.yml\nProcure por command: e dentro deste bloco, substitua o texto do placeholder \u0026lt;AUTOSCALING GROUP NAME\u0026gt; com o nome do ASG que você copiou na etapa anterior. Além disso, atualize o valor AWS_REGION para refletir a região que você está usando e **Salve ** o arquivo.\ncommand: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=aws - --skip-nodes-with-local-storage=false - --nodes=2:8:eksctl-eksworkshop-eksctl-nodegroup-0-NodeGroup-SQG8QDVSR73G env: - name: AWS_REGION value: us-east-1  Este comando contém toda a configuração do Autoescalador do cluster. A configuração principal é a flag --nodes . Isso especifica o mínimo de nodes (2), máximo de nodes (8) e Nome do ASG.\nEmbora o Autoescalador de cluster seja o padrão de fato para o dimensionamento automático em K8s, não faz parte do release principal. Nós o implantamos como qualquer outro pod no namespac do kube-system, semelhante a outros pods de gerenciamento.\nCrie uma política do IAM Precisamos configurar uma política embutida e adicioná-la ao perfil da instância do EC2 do worker nodes\nColete o perfil da instância e o NOME da role na stack do CloudFormation\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r .Stacks[].StackName | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName')  mkdir ~/environment/asg_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/asg_policy/k8s-asg-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingInstances\u0026quot;, \u0026quot;autoscaling:SetDesiredCapacity\u0026quot;, \u0026quot;autoscaling:TerminateInstanceInAutoScalingGroup\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker --policy-document file://~/environment/asg_policy/k8s-asg-policy.json  Valide se a política está anexada a role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker  Implantar o autoescalador do cluster kubectl apply -f ~/environment/cluster-autoscaler/cluster_autoscaler.yml  Acompanhe os logs\nkubectl logs -f deployment/cluster-autoscaler -n kube-system  Agora estamos prontos para escalar nosso cluster   Related files   cluster_autoscaler.yml  (3 ko)    "
},
{
	"uri": "/logging/deploy/",
	"title": "Implantar Fluentd",
	"tags": [],
	"description": "",
	"content": "mkdir ~/environment/fluentd cd ~/environment/fluentd wget https://eksworkshop.com/logging/deploy.files/fluentd.yml  Explore o fluentd.yml para ver o que está sendo implantado. Existe um link na parte inferior desta página. A configuração do agente de log do Fluentd está localizada no Kubernetes ConfigMap. O Fluentd será implantado como um DaemonSet, ou seja, um pod por work node. No nosso caso, um cluster de 3 node é usado e, portanto, serão exibidos 3 pods na saída quando implantarmos.\nAtualize a REGION no fluentd.yml conforme necessário. Está definido para us-west-2 por padrão.\n kubectl apply -f ~/environment/fluentd/fluentd.yml  Cuidado com todos os pods para mudar para o status running\nkubectl get pods -w --namespace=kube-system  Agora estamos prontos para verificar se os logs estão chegando CloudWatch Logs\nSelecione a região mencionada em fluentd.yml para procurar o Grupo de Logs do Cloudwatch, se necessário.\n  Related files   fluentd.yml  (5 ko)    "
},
{
	"uri": "/helm_root/helm_micro/deploy/",
	"title": "Implantar o eksdemo Chart",
	"tags": [],
	"description": "",
	"content": " Use a flag dry-run para testar nossos templates Para testar a sintaxe e a validade do chart sem realmente implementá-lo, usaremos a flag dry-run .\nO comando a seguir criará e exibirá os templates renderizados sem instalar oChart:\nhelm install --debug --dry-run --name workshop ~/environment/eksdemo  Confirme se os valores criados pelo template estão corretos.\nImplante o chart Agora que testamos nosso template, vamos instalá-lo.\nhelm install --name workshop ~/environment/eksdemo  Semelhante ao que vimos anteriormente no NGINX Helm Chart example, uma saída dos objetos Deployment, Pod e Service é gerada, semelhante a essa:\nNAME: workshop LAST DEPLOYED: Fri Nov 16 21:42:00 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME AGE ecsdemo-crystal 0s ecsdemo-frontend 0s ecsdemo-nodejs 0s ==\u0026gt; v1/Deployment ecsdemo-crystal 0s ecsdemo-frontend 0s ecsdemo-nodejs 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE ecsdemo-crystal-764b9cb9bc-4dwqt 0/1 ContainerCreating 0 0s ecsdemo-crystal-764b9cb9bc-hcb62 0/1 ContainerCreating 0 0s ecsdemo-crystal-764b9cb9bc-vl7nr 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-2xrtb 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-bfnc5 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-rb6rg 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-994cq 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-9qtbm 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-s9zkh 0/1 ContainerCreating 0 0s  "
},
{
	"uri": "/statefulset/testfailure/",
	"title": "Teste de falha",
	"tags": [],
	"description": "",
	"content": " container Pouco saudável MySQL container uses readiness probe by running mysql -h 127.0.0.1 -e \u0026lsquo;SELECT 1\u0026rsquo; on the server to make sure MySQL server is still active. Open a new terminal and simulate MySQL as being unresponsive by following command.\nkubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off  This command renames the /usr/bin/mysql command so that readiness probe can\u0026rsquo;t find it. During the next health check, the pod should report one of it\u0026rsquo;s containers is not healthy. This can be verified by following command.\nkubectl get pod mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 1/2 Running 0 12m  mysql-read load balancer detects failures and takes action by not sending traffic to the failed container, @@server_id 102. You can check this by the loop running in separate window from previous section. The loop shows the following output.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:00:45 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:46 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:00:47 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:48 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:49 | +-------------+---------------------+  Revert back to its initial state at the previous terminal.\nkubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql  Check the status again to see that both containers are running and healthy\n$ kubectl get pod -w mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 2/2 Running 0 5h  The loop in another terminal is now showing @@server_id 102 is back and all three servers are running. Press Ctrl+C to stop watching.\nFailed pod To simulate a failed pod, delete mysql-2 pod by following command.\nkubectl delete pod mysql-2  pod \u0026quot;mysql-2\u0026quot; deleted  StatefulSet controller recognizes failed pod and creates a new one to maintain the number of replicas with them same name and link to the same PersistentVolumeClaim.\n$ kubectl get pod -w mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 2/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Pending 0 0s mysql-2 0/2 Pending 0 0s mysql-2 0/2 Init:0/2 0 0s mysql-2 0/2 Init:1/2 0 10s mysql-2 0/2 PodInitializing 0 11s mysql-2 1/2 Running 0 12s mysql-2 2/2 Running 0 16s  Press Ctrl+C to stop watching.\n"
},
{
	"uri": "/prerequisites/workspaceiam/",
	"title": "Atualizar as configurações do IAM para sua área de trabalho",
	"tags": [],
	"description": "",
	"content": " O Cloud9 normalmente gerencia as credenciais do IAM dinamicamente. No momento, isso não é compatível com o plug-in aws-iam-authenticator, por isso, vamos desativá-lo e confiar na role do IAM.\n  Retorne ao seu espaço de trabalho e clique na roda dentada ou inicie uma nova guia para abrir a guia Preferências. Selecione AWS SETTINGS Desligar A AWS gerencia credenciais temporárias Feche a aba Preferências  Para garantir que as credenciais temporárias ainda não estejam em vigor, também removeremos qualquer arquivo de credenciais existente:\nrm -vf ${HOME}/.aws/credentials  Nós devemos configurar nosso aws cli com nossa região atual como padrão:\nexport AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r .region) echo \u0026quot;export AWS_REGION=${AWS_REGION}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region   Validar a Role do IAM Use o comando da CLI GetCallerIdentity para validar se o Cloud9 IDE está usando o IAM role.\nPrimeiro, obtenha o nome da Role do IAM na AWS CLI.\nINSTANCE_PROFILE_NAME=`basename $(aws ec2 describe-instances --filters Name=tag:Name,Values=aws-cloud9-${C9_PROJECT}-${C9_PID} | jq -r '.Reservations[0].Instances[0].IamInstanceProfile.Arn' | awk -F \u0026quot;/\u0026quot; \u0026quot;{print $2}\u0026quot;)` aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME --query \u0026quot;InstanceProfile.Roles[0].RoleName\u0026quot; --output text  A saída é o nome da role.\nmodernizer-workshop-cl9  Compare isso com o resultado de\naws sts get-caller-identity  VALID Se o Arn contiver o nome da role acima e um ID da instância, você poderá continuar.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/modernizer-workshop-cl9/i-01234567890abcdef\u0026quot; }  INVALID Se o _Arn contiver TeamRole, MasterRole, ou não corresponde ao nome da role, DO NOT PROCEED. Volte e confirme as etapas nesta página.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/TeamRole/MasterRole\u0026quot; }  "
},
{
	"uri": "/dashboard/",
	"title": "Implantar o Dashboard do Kubernetes",
	"tags": [],
	"description": "",
	"content": " Implantar o Dashboard do Kubernetes Neste capítulo, vamos implantar o Dashboard oficial do Kubernetes e conectar através de nosso Workspace Cloud9 .\n"
},
{
	"uri": "/deploy/",
	"title": "Implantar os microserviços de exemplo",
	"tags": [],
	"description": "",
	"content": " Implantar os microserviços de exemplo  Faça deploy de nossos aplicativos de exemplo   Implantar a API de back-end do NodeJS   Implantar a API do Crystal Backend   Vamos checar tipos de serviço   Assegure-se de que a função de serviço do ELB exista   Implantar o serviço de front-end   Encontre o endereço do serviço   Escale os serviços de back-end   Escale o frontend   Limpar os aplicativos   "
},
{
	"uri": "/spotworkers/",
	"title": "Using Spot Instances with EKS",
	"tags": [],
	"description": "",
	"content": " Using Spot Instances with EKS In this module, you will learn how to provision, manage, and maintain your Kubernetes clusters with Amazon EKS at any scale on Spot Instances to optimize cost and scale.\n"
},
{
	"uri": "/helm_root/",
	"title": "Helm",
	"tags": [],
	"description": "",
	"content": " Kubernetes Helm Helm é um gerenciador de pacotes para o Kubernetes que empacota vários recursos do Kubernetes em uma única unidade de implementação lógica chamada Chart.\nNeste capítulo, abordaremos instalando o Helm. Uma vez instalado, vamos demonstrar como o Helm pode ser usado para implantar um servidor da Web simples do NGINX, e mais sophisticated microservice.\n"
},
{
	"uri": "/statefulset/testscaling/",
	"title": "Teste de escalonamento",
	"tags": [],
	"description": "",
	"content": " Mais slaves podem ser adicionados ao MySQL Cluster para aumentar a capacidade de leitura. Isso pode ser feito seguindo o comando.\nkubectl scale statefulset mysql --replicas=5  Você pode ver a mensagem que statefulset \u0026lsquo;mysql\u0026rsquo; escalou.\nstatefulset \u0026quot;mysql\u0026quot; scaled  Veja o progresso da escala ordenada e graciosa.\nkubectl get pods -l app=mysql -w  NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 1d mysql-1 2/2 Running 0 1d mysql-2 2/2 Running 0 24m mysql-3 0/2 Init:0/2 0 8s mysql-3 0/2 Init:1/2 0 9s mysql-3 0/2 PodInitializing 0 11s mysql-3 1/2 Running 0 12s mysql-3 2/2 Running 0 16s mysql-4 0/2 Pending 0 0s mysql-4 0/2 Pending 0 0s mysql-4 0/2 Init:0/2 0 0s mysql-4 0/2 Init:1/2 0 10s mysql-4 0/2 PodInitializing 0 11s mysql-4 1/2 Running 0 12s mysql-4 2/2 Running 0 17s  Pode demorar alguns minutos para iniciar todos os pods.\n Pressione Ctrl C para parar de acompanhar. Abra outro terminal para Verificar se você fechou o loop .\nkubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\ bash -ic \u0026quot;while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done\u0026quot;  You will see 5 servers are running.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:56:42 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 102 | 2018-11-14 13:56:43 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 104 | 2018-11-14 13:56:44 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:56:45 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 104 | 2018-11-14 13:56:46 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:56:47 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:56:48 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 103 | 2018-11-14 13:56:49 | +-------------+---------------------+  Verifique se o slave recém-implementado (mysql-3) possui o mesmo conjunto de dados pelo comando a seguir.\nkubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\ mysql -h mysql-3.mysql -e \u0026quot;SELECT * FROM test.messages\u0026quot;  Ele mostrará os mesmos dados que o master possui.\n+--------------------------+ | message | +--------------------------+ | hello, from mysql-client | +--------------------------+  Reduza as réplicas para 3 executando o comando.\nkubectl scale statefulset mysql --replicas=3  Você pode ver o \u0026lsquo;mysql\u0026rsquo; statefulset escalonado\nstatefulset \u0026quot;mysql\u0026quot; scaled  Observe que a escala não exclui os dados ou PVCs anexados aos pods. Você tem que excluí-los manualmente. Verifique se a escala é finalizada pelo seguinte comando.\nkubectl get pods -l app=mysql  NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 1d mysql-1 2/2 Running 0 1d mysql-2 2/2 Running 0 35m  Verifique 2 PVCs (data-mysql-3, data-mysql-4) ainda existem executando o comando.\nkubectl get pvc -l app=mysql  NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-mysql-0 Bound pvc-83e9dfeb-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-1 Bound pvc-977e7806-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-2 Bound pvc-b3009b02-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-3 Bound pvc-de14acd8-e811-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 34m data-mysql-4 Bound pvc-e916c3ec-e812-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 26m  Desafio: Por padrão, a exclusão de um PersistentVolumeClaim excluirá seu volume persistente associado. E se você quisesse manter o volume? Alterar a política de recuperação do PersistentVolume associado ao PVC \u0026ldquo;data-mysql-3\u0026rdquo; para \u0026ldquo;Retain\u0026rdquo;. Por favor, veja Documentação do kubernetes para ajudá-lo\n  Expand here to see the solution   Alterar a política de reclaim:\nkubectl patch pv \u0026lt;your-pv-name\u0026gt; -p '{\u0026quot;spec\u0026quot;:{\u0026quot;persistentVolumeReclaimPolicy\u0026quot;:\u0026quot;Retain\u0026quot;}}'  Agora, se você excluir o Data-mysql-3 do PersistentVolumeClaim, você ainda pode ver o volume do EBS no seu console do AWS EC2, com seu status \u0026ldquo;available\u0026rdquo;.\nVamos alterar a política de reclaim de volta para \u0026lsquo;Excluir\u0026rsquo; para evitar volumes órfãos:\nkubectl patch pv \u0026lt;your-pv-name\u0026gt; -p '{\u0026quot;spec\u0026quot;:{\u0026quot;persistentVolumeReclaimPolicy\u0026quot;:\u0026quot;Delete\u0026quot;}}'    Delete data-mysql-3, data-mysql-4 by following command.\nkubectl delete pvc data-mysql-3 kubectl delete pvc data-mysql-4  persistentvolumeclaim \u0026quot;data-mysql-3\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-4\u0026quot; deleted  "
},
{
	"uri": "/batch/deploy/",
	"title": "Deploy Argo",
	"tags": [],
	"description": "",
	"content": " Deploy Argo Argo run in its own namespace and deploys as a CustomResourceDefinition.\nDeploy the Controller and UI.\nkubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/v2.2.1/manifests/install.yaml  namespace/argo created customresourcedefinition.apiextensions.k8s.io/workflows.argoproj.io created serviceaccount/argo created serviceaccount/argo-ui created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-admin created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-edit created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-view created clusterrole.rbac.authorization.k8s.io/argo-cluster-role created clusterrole.rbac.authorization.k8s.io/argo-ui-cluster-role created clusterrolebinding.rbac.authorization.k8s.io/argo-binding created clusterrolebinding.rbac.authorization.k8s.io/argo-ui-binding created configmap/workflow-controller-configmap created service/argo-ui created deployment.apps/argo-ui created deployment.apps/workflow-controller created  To use advanced features of Argo for this demo, create a RoleBinding to grant admin privileges to the \u0026lsquo;default\u0026rsquo; service account.\nThis is for demo purposes only. In any other environment, you should use Workflow RBAC to set appropriate permissions.\n kubectl create rolebinding default-admin --clusterrole=admin --serviceaccount=default:default  "
},
{
	"uri": "/servicemesh/deploy/",
	"title": "Implantar aplicativos de exemplo",
	"tags": [],
	"description": "",
	"content": " Agora que temos todos os recursos instalados para o Istio, usaremos um aplicativo de exemplo chamado BookInfo para analisar os principais recursos da service mesh, como o roteamento inteligente, e analisar os dados de telemetria usando o Prometheus.\nSample Apps O aplicativo Bookinfo é dividido em quatro microsserviços separados:\n productpage\n O microsserviço da página do produto chama os detalhes e revisa microsserviços para preencher a página.  details\n O microsserviço de detalhes contém informações do livro.  reviews\n O microserviço de avaliações contém resenhas de livros. Ele também chama o microsserviço de classificações.  ratings\n O microsserviço de classificações contém informações de classificação de livros que acompanham uma resenha do livro.   Existem 3 versões do reviewsmicrosserviço:\n Version v1\n não chama o serviço de classificações.  Version v2\n chama o serviço de classificação e exibe cada classificação como 1 a 5black stars.  Version v3\n chama o serviço de classificação e exibe cada classificação como 1 a 5 red stars.   Implantar aplicativos de exemplo Implante aplicativos de exemplo injetando manualmente o istio proxy e confirme os pods, os serviços estão sendo executados corretamente\nkubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)  Para verificar o resultado\nkubectl get pod,svc  Deve ser semelhante a:\nNAME READY STATUS RESTARTS AGE details-v1-64558cf56b-dxbx2 2/2 Running 0 14s productpage-v1-5b796957dd-hqllk 2/2 Running 0 14s ratings-v1-777b98fcc4-5bfr8 2/2 Running 0 14s reviews-v1-866dcb7ff-k69jm 2/2 Running 0 14s reviews-v2-6d7959c9d-5ppnc 2/2 Running 0 14s reviews-v3-7ddf94f545-m7vls 2/2 Running 0 14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.100.102.153 \u0026lt;none\u0026gt; 9080/TCP 17s kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 138d productpage ClusterIP 10.100.222.154 \u0026lt;none\u0026gt; 9080/TCP 17s ratings ClusterIP 10.100.1.63 \u0026lt;none\u0026gt; 9080/TCP 17s reviews ClusterIP 10.100.255.157 \u0026lt;none\u0026gt; 9080/TCP 17s  Em seguida, vamos definir o serviço virtual e o gateway de ingress:\nkubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml  Em seguida, vamos consultar o nome DNS do gateway de ingress e usá-lo para se conectar através do navegador.\nkubectl get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' -n istio-system ; echo  Isso pode levar um ou dois minutos, primeiro para o Ingress ser criado e, em segundo lugar, para o Ingress conectar-se aos serviços que ele expõe.\nPara testar, faça o seguinte:\n1. Abra uma nova guia do navegador 2. Cole o endpoint DNS retornado do comando anterior get service istiogateway 3. Adicionar /productpage ao final desse DNS endpoint 4. Pressione Enter para recuperar a página.\nLembre-se de adicionar /productpage até o final do URI no navegador para ver a página de exemplo!\n Clique em recarregar várias vezes para ver como o layout e o conteúdo das revisões são alterados como versões diferentes (v1, v2, v3) do aplicativo são chamados.\n"
},
{
	"uri": "/introduction/basics/concepts_nodes/",
	"title": "Nodes do Kubernetes",
	"tags": [],
	"description": "",
	"content": "As máquinas que formam um cluster Kubernetes são chamadas nodes.\nOs nós em um cluster do Kubernetes podem ser físicos ou virtuais.\nExistem dois tipos de nós:\n Um tipo Master-node, que compõe o Plano de controle, age como o \u0026lsquo;cérebro\u0026rsquo; do cluster.\n um tipo Worker-node , que compõe o Plano de dados, executa as imagens reais do contêiner (via pods.)\n  Vamos mergulhar mais fundo na forma como os nós interagem uns com os outros mais tarde na apresentação.\n"
},
{
	"uri": "/deploy/viewservices/",
	"title": "Encontre o endereço do serviço",
	"tags": [],
	"description": "",
	"content": "Agora que temos um serviço em execução que é type: LoadBalancer, precisamos encontrar o endereço do ELB. Podemos fazer isso usando a operação get services do kubectl:\nkubectl get service ecsdemo-frontend  Observe que o campo não é largo o suficiente para mostrar o FQDN do ELB. Podemos ajustar o formato de saída com este comando:\nkubectl get service ecsdemo-frontend -o wide  Se quiséssemos usar os dados pragmaticamente, também poderíamos produzir via json. Este é um exemplo de como podemos usar a saída json:\nELB=$(kubectl get service ecsdemo-frontend -o json | jq -r '.status.loadBalancer.ingress[].hostname') curl -m3 -v $ELB  Levará vários segundos para o ELB ficar saudável e começar a passar o tráfego para os pods do frontend.\n Você também deve poder copiar/colar o nome do host loadBalancer em seu navegador e ver o aplicativo em execução. Mantenha esta aba aberta enquanto escalamos os serviços na próxima página.\n"
},
{
	"uri": "/terraform/kubeconfig/",
	"title": "Create Kubeconfig File",
	"tags": [],
	"description": "",
	"content": "Agora que temos o cluster em execução, precisamos criar o arquivo KubeConfig que será usado para gerenciar o cluster.\nO módulo terraform armazena as informações do kubeconfig em seu armazenamento de estado. Podemos visualizá-lo com este comando:\nterraform output kubeconfig  E podemos salvá-lo para uso com este comando:\nterraform output kubeconfig \u0026gt; ${HOME}/.kube/config-eksworkshop-tf  Agora precisamos adicionar essa nova configuração à lista de configuração do KubeCtl:\nexport KUBECONFIG=${HOME}/.kube/config-eksworkshop-tf:${HOME}/.kube/config echo \u0026quot;export KUBECONFIG=${KUBECONFIG}\u0026quot; \u0026gt;\u0026gt; ${HOME}/.bashrc  "
},
{
	"uri": "/logging/configurecwl/",
	"title": "Configurar logs do CloudWatch e Kibana",
	"tags": [],
	"description": "",
	"content": " Configurar Subscrição de Logs do CloudWatch Os logs do CloudWatch podem ser entregues a outros serviços, como o Amazon Elasticsearch, para processamento personalizado. Isso pode ser obtido assinando um feed em tempo real de eventos de log. Um filtro de assinatura define o padrão de filtro a ser usado para filtrar quais eventos de log são entregues ao Elasticsearch, bem como informações sobre para onde enviar eventos de log correspondentes.\nNesta seção, inscreveremos os eventos de log do CloudWatch do fluxo fluent-cloudwatch do grupo de logs eks/eksworkshop-eksctl. Esse feed será transmitido para o cluster do Elasticsearch.\nAs instruções originais para isso estão disponíveis em:\nhttp://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html\nCrie a role de execução básica do Lambda\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/iam_policy/lambda.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } EoF aws iam create-role --role-name lambda_basic_execution --assume-role-policy-document file://~/environment/iam_policy/lambda.json aws iam attach-role-policy --role-name lambda_basic_execution --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole  Vá ao console de logs CloudWatch\nSelecione o grupo de log /eks/eksworkshop-eksctl/containers. Clique em Actions e selecioneStream to Amazon ElasticSearch Service. Selecione o cluster do ElasticSearch kubernetes-logs e a role do IAM lambda_basic_execution\nClique em \u0026lsquo;Next\u0026rsquo;\nSelecione Common Log Format e clique emNext\nRevise a configuração. Clique em Next e depois em \u0026lsquo;Start Streaming\u0026rsquo;\nA página do Cloudwatch é atualizada para mostrar que o filtro foi criado com sucesso\nConfigurar o Kibana No console do Amazon Elasticsearch, selecione o kubernetes-logs sob Meus domínios\nAbra o dashboard do Kibana no link. Após alguns minutos, os logs começarão a ser indexados pelo ElasticSearch. Você precisará configurar um padrão de índice no Kibana.\nConfigure Index Pattern como cwl-* e clique Next\nSelect @timestamp from the dropdown list and select Create index pattern\nClique em \u0026lsquo;Discover\u0026rsquo; e explore seus logs\n"
},
{
	"uri": "/scaling/test_ca/",
	"title": "Dimensionar um cluster com CA",
	"tags": [],
	"description": "",
	"content": " Implantar um aplicativo de exemplo Vamos implantar um exemplo de aplicativo nginx como ReplicaSet of 1 Pod\ncat \u0026lt;\u0026lt;EoF\u0026gt; ~/environment/cluster-autoscaler/nginx.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-to-scaleout spec: replicas: 1 template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: nginx-to-scaleout resources: limits: cpu: 500m memory: 512Mi requests: cpu: 500m memory: 512Mi EoF kubectl apply -f ~/environment/cluster-autoscaler/nginx.yaml kubectl get deployment/nginx-to-scaleout  Escalar nosso ReplicaSet OK, vamos escalar o replicaset para 10\nkubectl scale --replicas=10 deployment/nginx-to-scaleout  Alguns pods estarão no estado Pending, que aciona o escalonamento automático do cluster para escalonar a frota do EC2.\nkubectl get pods -o wide --watch  NAME READY STATUS RESTARTS AGE nginx-to-scaleout-7cb554c7d5-2d4gp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-2nh69 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-45mqz 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-4qvzl 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5jddd 1/1 Running 0 34s nginx-to-scaleout-7cb554c7d5-5sx4h 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5xbjp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-6l84p 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-7vp7l 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-86pr6 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-88ttw 0/1 Pending 0 12s  Visualizar os logs do cluster autoscaler\nkubectl logs -f deployment/cluster-autoscaler -n kube-system  Você observará eventos do Autoescalador de cluster semelhantes ao abaixo Verifique o AWS Management Console para confirmar se os grupos do Auto Scaling estão sendo dimensionados para atender à demanda. Isso pode levar alguns minutos. Você também pode acompanhar a implantação do pod a partir da linha de comando. Você deve ver a transição dos pods de pendente para execução, à medida que os nós são dimensionados.\n"
},
{
	"uri": "/statefulset/cleanup/",
	"title": "Limpar",
	"tags": [],
	"description": "",
	"content": " Primeiro exclua o StatefulSet. Isso também excluirá os pods. Pode demorar um pouco.\nkubectl delete statefulset mysql  Verifique se não há pods em execução executando o comando.\nkubectl get pods -l app=mysql  No resources found.  Exclua o ConfigMap, Service e PVC, executando o comando.\nkubectl delete configmap,service,pvc -l app=mysql  configmap \u0026quot;mysql-config\u0026quot; deleted service \u0026quot;mysql\u0026quot; deleted service \u0026quot;mysql-read\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-0\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-1\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-2\u0026quot; deleted  Parabéns! Você terminou o laboratório StatefulSets. "
},
{
	"uri": "/helm_root/helm_micro/service/",
	"title": "Teste o serviço",
	"tags": [],
	"description": "",
	"content": "Para testar o serviço criado pelo nosso eksdemo Chart, precisaremos obter o nome do endpoint do ELB que foi gerado quando implantamos o Chart:\nkubectl get svc ecsdemo-frontend -o jsonpath=\u0026quot;{.status.loadBalancer.ingress[*].hostname}\u0026quot;; echo  Copie esse endereço e cole-o em uma nova guia no seu navegador. Você deve ver algo semelhante a:\n"
},
{
	"uri": "/healthchecks/",
	"title": "Health Checks",
	"tags": [],
	"description": "",
	"content": " Health Checks Por padrão, o Kubernetes irá reiniciar um contêiner se ele falhar por algum motivo. Ele usa os probes Liveness e Readiness, que podem ser configurados para executar um aplicativo robusto, identificando os contêineres saudáveis ​​para enviar tráfego e reiniciando os quando necessário.\nNesta seção, vamos entender como liveness e readiness probes são definidos e testam o mesmo contra estados diferentes de um pod. Abaixo está a descrição de alto nível de como estas sondas funcionam.\nLiveness probes são usados ​​em Kubernetes para saber quando um pod está vivo ou morto. Um pod pode estar em um estado morto por diferentes razões, enquanto o Kubernetes mata e recria o pod quando a sonda de liveness não passa.\nReadiness probes são usados ​​no Kubernetes para saber quando um pod está pronto para atender ao tráfego. Somente quando o probe de prontidão passar, um pod receberá tráfego do serviço. Quando o probe de prontidão falhar, o tráfego não será enviado a um pod até que seja aprovado.\nVamos revisar alguns exemplos neste módulo para entender as diferentes opções de configuração de probes de disponibilidade e disponibilidade.\n"
},
{
	"uri": "/scaling/",
	"title": "Autoscaling our Applications and Clusters",
	"tags": [],
	"description": "",
	"content": " Implement AutoScaling with HPA and CA Neste capítulo, mostraremos padrões para dimensionar automaticamente os node worker e as implantações de aplicativos. O escalonamento automático no K8s vem em duas formas:\n Horizontal Pod Autoscaler (HPA) dimensiona os pods em um conjunto de implantação ou réplica. Ele é implementado como um recurso de API do K8s e um controlador. O gerenciador do controlador consulta a utilização de recursos em relação às métricas especificadas em cada definição de HorizontalPodAutoscaler. Obtém as métricas da API de métricas de recursos (para métricas de recurso por pod) ou da API de métricas personalizadas (para todas as outras métricas).\n Cluster Autoscaler (CA) é o componente K8s padrão que pode ser usado para executar dimensionamento de pod e dimensionar nós em um cluster. Aumenta automaticamente o tamanho de um grupo Auto Scaling para que os pods tenham um lugar para ser executado. E tenta remover nós inativos, ou seja, nós sem pods em execução.\n  "
},
{
	"uri": "/codepipeline/",
	"title": "CI/CD com CodePipeline",
	"tags": [],
	"description": "",
	"content": "# CI/CD com CodePipeline\nContinuous integration (CI) e continuous delivery (CD) são essenciais para organizações hábeis. As equipes são mais produtivas quando podem fazer alterações discretas com frequência, liberar essas alterações programaticamente e entregar atualizações sem interrupções.\nNeste módulo, construiremos um pipeline de CI/CD usando AWS CodePipeline. O pipeline de CI/CD implantará um serviço de exemplo do Kubernetes, faremos uma alteração no repositório do GitHub e observaremos a entrega automatizada dessa mudança no cluster.\n"
},
{
	"uri": "/x-ray/",
	"title": "Tracing with X-Ray",
	"tags": [],
	"description": "",
	"content": " Tracing with X-Ray As distributed systems evolve, monitoring and debugging services becomes challenging. Container-orchestration platforms like Kubernetes solve a lot of problems, but they also introduce new challenges for developers and operators in understanding how services interact and where latency exists. AWS X-Ray helps developers analyze and debug distributed services.\nIn this module, we are going to deploy the X-Ray agent as a DaemonSet, deploy sample front-end and back-end services that are instrumented with the X-Ray SDK, make some sample requests and then examine the traces and service maps in the AWS Management Console.\n"
},
{
	"uri": "/batch/",
	"title": "Batch Processing with Argo",
	"tags": [],
	"description": "",
	"content": " Batch Processing Neste capítulo, vamos implantar cenários comuns de processamento em lote usando Kubernetes e Argo.\nO que é Argo? O Argo é um mecanismo de fluxo de trabalho nativo de contêiner de software livre para executar o trabalho no Kubernetes.\n Definir fluxos de trabalho em que cada etapa do fluxo de trabalho é um contêiner. Modele fluxos de trabalho de várias etapas como uma sequência de tarefas ou capture as dependências entre tarefas usando um gráfico (DAG). Execute facilmente tarefas intensivas de computação para aprendizado de máquina ou processamento de dados em uma fração do tempo usando fluxos de trabalho Argo no Kubernetes.  "
},
{
	"uri": "/batch/artifact/",
	"title": "Configurar Repositório de Artefato",
	"tags": [],
	"description": "",
	"content": " Configurar Repositório de Artefato O Argo usa um repositório de artefatos para passar dados entre tarefas em um fluxo de trabalho, conhecido como artefatos.\nVamos criar um bucket S3 usando o AWS CLI.\nACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) aws s3 mb s3://batch-artifact-repository-${ACCOUNT_ID}/  Next, edit the workflow-controller ConfigMap to use the S3 bucket.\nkubectl edit -n argo configmap/workflow-controller-configmap  Add the following lines to the end of the ConfigMap, substituting your Account ID for {{ACCOUNT_ID}}:\ndata: config: | artifactRepository: s3: bucket: batch-artifact-repository-{{ACCOUNT_ID}} endpoint: s3.amazonaws.com  Create an IAM Policy In order for Argo to read from/write to the S3 bucket, we need to configure an inline policy and add it to the EC2 instance profile of the worker nodes.\nCollect the Instance Profile, Role name, and Account ID from the CloudFormation Stack.\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r .Stacks[].StackName | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)  Create and policy and attach to the worker node role.\nmkdir ~/environment/batch_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/batch_policy/k8s-s3-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:*\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::batch-artifact-repository-${ACCOUNT_ID}\u0026quot;, \u0026quot;arn:aws:s3:::batch-artifact-repository-${ACCOUNT_ID}/*\u0026quot; ] } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker --policy-document file://~/environment/batch_policy/k8s-s3-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker  "
},
{
	"uri": "/servicemesh/routing/",
	"title": "Roteamento Inteligente",
	"tags": [],
	"description": "",
	"content": " Roteamento Inteligente A implantação de um aplicativo baseado em microsserviço em uma service mesh do Istio permite controlar externamente o monitoramento e o rastreamento de serviços, o roteamento de solicitação (versão), testes de resiliência, segurança e aplicação de políticas e mais de maneira consistente em todos os serviços e no aplicativo.\nAntes de usar o Istio para controlar o roteamento da versão Bookinfo, você precisará definir as versões disponíveis, chamadas subsets, nas regras de destino.\nVersões de serviço (a.k.a. subsets) - Em um cenário de implantação contínua, para um determinado serviço, pode haver subconjuntos distintos de instâncias executando diferentes variantes do binário do aplicativo. Essas variantes não são necessariamente versões diferentes da API. Podem ser alterações iterativas para o mesmo serviço, implantadas em diferentes ambientes (prod, staging, dev, etc.). Cenários comuns onde isso ocorre incluem A/B testing, canary rollouts, etc. A escolha de uma determinada versão pode ser decidida com base em vários critérios (cabeçalhos, url, etc.) e / ou por pesos atribuídos a cada versão. Cada serviço tem uma versão padrão que consiste em todas as suas instâncias.\n kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml kubectl get destinationrules -o yaml  Para rotear para apenas uma versão, você aplica serviços virtuais que definem a versão padrão dos microsserviços. Nesse caso, os serviços virtuais rotearão todo o tráfego parareviews:v1 de microserviço.\nkubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl get virtualservices reviews -o yaml  O subconjunto é definido como v1 para todas as solicitações de comentários.\nspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1  Tente agora recarregar a página várias vezes e observe como somente a versão 1 das resenhas é exibida a cada vez.\nEm seguida, alteraremos a configuração da rota para que todo o tráfego de um usuário específico seja roteado para uma versão de serviço específica. Nesse caso, todo o tráfego de um usuário chamado Jason será encaminhado para o serviço reviews:v2.\nkubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml kubectl get virtualservices reviews -o yaml  O subconjunto é definido como v1 no padrão e rota v2 se o usuário logado for compatível com \u0026lsquo;jason\u0026rsquo; para solicitação de comentários.\nspec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1  Para testar, clique Sign in no canto superior direito da página e faça o login usando jason como nome de usuário com uma senha em branco. Você só verá resenhas: v2 o tempo todo. Outros verão revisões: v1.\nPara testar a resiliência, injete um atraso de 7 s entre os comentários: v2 e microsserviços de classificação para o usuário jason. Este teste irá descobrir um bug que foi intencionalmente introduzido no aplicativo Bookinfo.\nkubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml kubectl get virtualservice ratings -o yaml  O subset é definido como v1 no padrão e adicionado 7s de atraso para toda a solicitação, se o usuário logado for compatível com \u0026lsquo;jason\u0026rsquo; para classificações.\nspec: hosts: - ratings http: - fault: delay: fixedDelay: 7s percent: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1  Logout e clique em Sign in no canto superior direito da página, usando jason como o nome de usuário com uma senha em branco. Você verá os atrasos e o erro de exibição será exibido nos comentários. Outros verão comentários sem erros.\nO tempo limite entre a página de produto e o serviço de comentários é de 6 segundos - codificado como 3s 1 para 6s no total.\nPara testar outra resiliência, introduza um abort de HTTP nos microsserviços de classificações para o usuário de teste jason. A página exibirá imediatamente “Ratings service is currently unavailable”\nkubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml kubectl get virtualservice ratings -o yaml  O subset é definido como v1 e, por padrão, retorna uma mensagem de erro de\u0026rdquo;Ratings service is currently unavailable\u0026rdquo; abaixo do nome do revisor se o nome de usuário registrado corresponder \u0026lsquo;jason\u0026rsquo;.\nspec: hosts: - ratings http: - fault: abort: httpStatus: 500 percent: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1  Para testar, clique Sign in no canto superior direito da página e faça o login usando jason para o nome de usuário com uma senha em branco. Como jason você verá a mensagem de erro. Outros (não logados como jason) não verão mensagem de erro.\nEm seguida, demonstraremos como migrar gradualmente o tráfego de uma versão de um microsserviço para outro. Em nosso exemplo, vamos enviar 50% de tráfego para reviews:v1e 50% para reviews:v3.\nkubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml kubectl get virtualservice reviews -o yaml  O subset está definido para 50% do tráfego para v1 e 50% do tráfego para v3 para todos os pedidos de comentários.\nspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 50 - destination: host: reviews subset: v3 weight: 50  Para testá-lo, atualize seu navegador várias vezes, e você verá apenas reviews:v1 e reviews:v3.\n"
},
{
	"uri": "/introduction/basics/concepts_objects/",
	"title": "Visão Geral dos Objetos K8s",
	"tags": [],
	"description": "",
	"content": "Objetos do Kubernetes são entidades usadas para representar o estado do cluster.\nUm objeto é um “registro de intenção” - uma vez criado, o cluster faz o possível para garantir que ele exista conforme definido. Isso é conhecido como o cluster “desired state.”\nO Kubernetes está sempre trabalhando para tornar o “Estado atual” de um objeto igual ao objeto “Estado desejado.” Um estado desejado pode descrever:\n Quais pods (contêineres) estão sendo executados e em quais nodes IP de endpoints que mapeiam para um grupo lógico de contêineres Quantas réplicas de um contêiner estão sendo executadas E muito mais\u0026hellip;  Vamos explicar esses objetos k8s com mais detalhes..\n"
},
{
	"uri": "/deploy/scalebackend/",
	"title": "Escale os serviços de back-end",
	"tags": [],
	"description": "",
	"content": "Quando lançamos nossos serviços, lançamos apenas um contêiner de cada um. Podemos confirmar isso visualizando os pods em execução:\nkubectl get deployments  Agora vamos escalar os serviços de back-end:\nkubectl scale deployment ecsdemo-nodejs --replicas=3 kubectl scale deployment ecsdemo-crystal --replicas=3  Confirme acompanhando o deployment novamente:\nkubectl get deployments  Além disso, verifique a guia do navegador onde podemos ver nosso aplicativo em execução. Agora você deve ver o tráfego fluindo para vários serviços de back-end.\n"
},
{
	"uri": "/terraform/configmap/",
	"title": "Crie o Worker ConfigMap",
	"tags": [],
	"description": "",
	"content": " O estado terraform também contém um configmap que podemos usar para o nosso EKS workers.\nVeja o configmap:\nterraform output config-map  Salve o config-map:\nterraform output config-map \u0026gt; /tmp/config-map-aws-auth.yml  Aplique o config-map:\nkubectl apply -f /tmp/config-map-aws-auth.yml  Confirme seu Nodes:\nkubectl get nodes  Parabéns! Agora você tem um cluster do Amazon EKS totalmente funcional pronto para uso!\n"
},
{
	"uri": "/scaling/cleanup/",
	"title": "Limpar Scaling",
	"tags": [],
	"description": "",
	"content": "INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r .Stacks[].StackName | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') kubectl delete -f ~/environment/cluster-autoscaler/cluster_autoscaler.yml kubectl delete -f ~/environment/cluster-autoscaler/nginx.yaml rm -rf ~/environment/cluster-autoscaler aws iam delete-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker kubectl delete hpa,svc php-apache kubectl delete deployment php-apache load-generator  "
},
{
	"uri": "/helm_root/helm_micro/rolling_back/",
	"title": "Retrocedendo",
	"tags": [],
	"description": "",
	"content": " Erros acontecerão durante a deploy e, quando isso acontecer, o Helm facilitará o desfazer ou \u0026ldquo;roll back\u0026rdquo; para a versão implementada anteriormente.\nAtualize o chart do aplicativo de demonstração com uma alteração de quebra Abrir values.yaml e modificar o nome da imagem em nodejs.image para brentley/ecsdemo-nodejs-non-existing. Esta imagem não existe, então isso vai quebrar a nosso deploy.\nFaça o Deploy do aplicativo de demonstração atualizado chart:\nhelm upgrade workshop ~/environment/eksdemo  A atualização contínua começará criando um novo pod de nodejs com a nova imagem. O novo Pod ecsdemo-nodejs deve deixar de puxar a imagem não existente. Execute o comando helm status para ver o erroImagePullBackOff:\nhelm status workshop  Reverter a atualização com falha Agora, vamos reverter o aplicativo para a revisão de versão anterior.\nPrimeiro, liste as revisões da versão do Helm:\nhelm history workshop  Em seguida, reverter para a revisão de aplicativo anterior (pode reverter para qualquer revisão também):\n# rollback to the 1st revision helm rollback workshop 1  Valide o status de release do workshop com:\nhelm status workshop  "
},
{
	"uri": "/logging/cleanup/",
	"title": "limpeza Centralizador de Logs",
	"tags": [],
	"description": "",
	"content": "cd ~/environment INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r .Stacks[].StackName | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') kubectl delete -f ~/environment/fluentd/fluentd.yml rm -rf ~/environment/fluentd/ aws es delete-elasticsearch-domain --domain-name kubernetes-logs aws logs delete-log-group --log-group-name /eks/eksworkshop-eksctl/containers aws iam delete-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker aws iam detach-role-policy --role-name lambda_basic_execution --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole aws iam delete-role --role-name lambda_basic_execution rm -rf ~/environment/iam_policy/  "
},
{
	"uri": "/calico/",
	"title": "Criar políticas de rede usando o Calico",
	"tags": [],
	"description": "",
	"content": " Criar políticas de rede usando o Calico Neste capítulo, criaremos algumas políticas de rede usando o Calico e veremos as regras em ação.\nAs políticas de rede permitem que você defina regras que determinam que tipo de tráfego pode fluir entre diferentes serviços. Usando políticas de rede, você também pode definir regras para restringir o tráfego. Eles são um meio de melhorar a segurança do seu cluster.\nPor exemplo, você só pode permitir o tráfego do frontend para o back-end em seu aplicativo.\nAs políticas de rede também ajudam a isolar o tráfego nos namespaces. Por exemplo, se você tiver namespaces separados para desenvolvimento e produção, poderá impedir o fluxo de tráfego entre eles, restringindo a comunicação do pod para o pod dentro do mesmo namespace.\n"
},
{
	"uri": "/spotworkers/cleanup/",
	"title": "Limpar",
	"tags": [],
	"description": "",
	"content": "Limpar nossa implantação de Microservices\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml  Limpar o Daemonset Manipulador de Manchas\nkubectl delete -f ~/environment/spot/spot-interrupt-handler-example.yml  Para limpar o worker criado por este módulo, execute os seguintes comandos:\nRemova os nodes Worker do EKS:\naws cloudformation delete-stack --stack-name \u0026quot;eksworkshop-spot-workers\u0026quot;  "
},
{
	"uri": "/cleanup/workspace/",
	"title": "Limpar o Workspace",
	"tags": [],
	"description": "",
	"content": "Vamos deletar nossa chave SSH:\naws ec2 delete-key-pair --key-name \u0026quot;eksworkshop\u0026quot;  Como não precisamos mais da instância Cloud9 para ter acesso de administrador à nossa conta, podemos excluir a role que criamos:\n Vamos para Console do IAM  Clique Delete a role no canto superior direito  Por fim, vamos excluir nossa instância do Cloud9 EC2:\n- Vá para suaCloud9 Environment - Selecione o ambiente chamado eksworkshop e escolha delete\n"
},
{
	"uri": "/logging/",
	"title": "Registrando logs com Elasticsearch, Fluentd, and Kibana (EFK)",
	"tags": [],
	"description": "",
	"content": " Implemente o log com o EFK Neste capítulo, vamos implantar um padrão de registro em log comum do Kubernetes, que consiste no seguinte:\n Fluentd é um coletor de dados de código aberto que fornece uma camada de registro unificada, suportada por 500 plugins conectados a vários tipos de sistemas. Elasticsearch é um mecanismo de pesquisa e análise distribuído e RESTful. Kibana permite visualizar seus dados do Elasticsearch.  Juntos, Fluentd, Elasticsearch e Kibana também são conhecidos como “stack” EFK . O Fluentd encaminhará os logs das instâncias individuais no cluster para um back-end de log centralizado (CloudWatch Logs), onde eles serão combinados para relatórios de nível mais alto usando o ElasticSearch e o Kibana.\n"
},
{
	"uri": "/monitoring/",
	"title": "Monitoring using Prometheus and Grafana",
	"tags": [],
	"description": "",
	"content": " Monitoring using Prometheus and Grafana In this Chapter, we will deploy Prometheus and Grafana to monitor Kubernetes cluster\n"
},
{
	"uri": "/servicemesh/",
	"title": "Service Mesh com Istio",
	"tags": [],
	"description": "",
	"content": " Service Mesh Um service mesh é uma camada de infraestrutura dedicada para manipulação service-to-service communication. Ele é responsável pela entrega confiável de solicitações por meio da complexa topologia de serviços que compõem um aplicativo nativo moderno na nuvem.\nAs soluções de malha de serviço têm dois componentes distintos que se comportam de maneira um pouco diferente: 1) um plano de dados e 2) um plano de controle. O diagrama a seguir ilustra a arquitetura básica.\n O plano de dados é composto por um conjunto de proxies inteligentes (Envoy) implementados como sidecars. Esses proxies servem de mediador e controlam toda a comunicação de rede entre microsserviços junto com o Mixer, uma política de propósito geral e hub de telemetria.\n Acontrol planegerencia e configura os proxies para rotear o tráfego. Além disso, o plano de controle configura Mixers para aplicar políticas e coletar telemetria.\n  "
},
{
	"uri": "/statefulset/",
	"title": "Contêineres com informações de estado usando StatefulSets",
	"tags": [],
	"description": "",
	"content": " Contêineres com informações de estado usando StatefulSets StatefulSets gerenciar deploy e o dimensionamento de um conjunto de pods e fornecer garantias sobre o pedido e a exclusividade desses pods, adequados para aplicativos que exigem um ou mais dos seguintes.\n Stable, unique network identifiers Stable, persistent storage Ordered, graceful deployment and scaling Ordered, automated rolling updates  Neste capítulo, revisaremos como implantar o banco de dados MySQL usando StatefulSets e EBS comoPersistentVolume. O exemplo é uma topologia de mestre único do MySQL com vários escravos executando replicação assíncrona.\n"
},
{
	"uri": "/batch/workflow-simple/",
	"title": "Simple Batch Workflow",
	"tags": [],
	"description": "",
	"content": " Simple Batch Workflow Save the below manifest as \u0026lsquo;workflow-whalesay.yaml\u0026rsquo; using your favorite editor and let\u0026rsquo;s deploy the whalesay example from before using Argo.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: whalesay- spec: entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay command: [cowsay] args: [\u0026quot;This is an Argo Workflow!\u0026quot;]  Now deploy the workflow using the argo CLI.\nYou can also run workflow specs directly using kubectl but the argo CLI provides syntax checking, nicer output, and requires less typing. For the equivalent kubectl commands, see Argo CLI.\n argo submit --watch workflow-whalesay.yaml  Name: whalesay-2kfxb Namespace: default ServiceAccount: default Status: Succeeded Created: Sat Nov 17 10:32:13 -0500 (3 seconds ago) Started: Sat Nov 17 10:32:13 -0500 (3 seconds ago) Finished: Sat Nov 17 10:32:16 -0500 (now) Duration: 3 seconds STEP PODNAME DURATION MESSAGE ✔ whalesay-2kfxb whalesay-2kfxb 2s  Make a note of the workflow\u0026rsquo;s name from your output (It should be similar to whalesay-xxxxx).\nConfirm the output by running the following command, substituting name of your workflow for \u0026ldquo;whalesay-xxxxx\u0026rdquo;:\nargo logs whalesay-xxxxx  ___________________________ \u0026lt; This is an Argo Workflow! \u0026gt; --------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  "
},
{
	"uri": "/servicemesh/visualize/",
	"title": "Monitorar &amp; Visualizar",
	"tags": [],
	"description": "",
	"content": " Coletando novos dados de telemetria Em seguida, faça o download de um arquivo YAML para manter a configuração da nova métrica e do fluxo de logs que o Istio irá gerar e coletar automaticamente.\ncurl -LO https://eksworkshop.com/servicemesh/deploy.files/istio-telemetry.yaml kubectl apply -f istio-telemetry.yaml  Certifique-se de que Prometheus e Grafana estejam funcionando\nkubectl -n istio-system get svc prometheus kubectl -n istio-system get svc grafana  Configurar o encaminhamento de porta para o Grafana executando o seguinte comando:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 8080:3000 \u0026amp;  Abra o Painel do Istio através da interface Grafana\n1. Em seu ambiente Cloud9, click Preview / Preview Running Application 1. Vá até the fim da URL e acrescente:\n/dashboard/db/istio-mesh-dashboard  Abra uma nova guia de terminal e insira para enviar um tráfego para o mesh\nexport SMHOST=$(kubectl get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname} ' -n istio-system) SMHOST=\u0026quot;$(echo -e \u0026quot;${SMHOST}\u0026quot; | tr -d '[:space:]')\u0026quot; while true; do curl -o /dev/null -s \u0026quot;${SMHOST}/productpage\u0026quot;; done  Você verá que o tráfego está distribuído uniformemente entre reviews:v1 e reviews:v3\nEncorajamos você a explorar outros painéis do Istio que estão disponíveis clicando em Istio Mesh Dashboard no menu do canto superior esquerdo da página, e selecionando um painel diferente.\n"
},
{
	"uri": "/introduction/basics/concepts_objects_details_1/",
	"title": "Detalhe de objetos do K8s(1/2)",
	"tags": [],
	"description": "",
	"content": " Pod  Uma unidade lógica de agrupamento em torno de um ou mais contêineres  DaemonSet  Implementa uma única instância de um pod em um worker node  Deployment  Detalhes sobre como distribuir (ou reverter) as versões do seu aplicativo  "
},
{
	"uri": "/deploy/scalefrontend/",
	"title": "Escale o frontend",
	"tags": [],
	"description": "",
	"content": "Vamos também dimensionar nosso serviço frontend da mesma forma:\nkubectl get deployments kubectl scale deployment ecsdemo-frontend --replicas=3 kubectl get deployments  Verifique a guia do navegador, onde podemos ver nosso aplicativo em execução. Agora você deve ver o tráfego fluindo para vários serviços frontend.\n"
},
{
	"uri": "/terraform/cleanup/",
	"title": "Limpar",
	"tags": [],
	"description": "",
	"content": "Para excluir os recursos criados para esse cluster EKS, execute os seguintes comandos:\nVeja o plan:\nterraform plan -destroy -out eksworkshop-destroy-tf  Execute o plan:\nterraform apply \u0026quot;eksworkshop-destroy-tf\u0026quot;  Destruir todos os recursos levará aproximadamente 15 minutos\n "
},
{
	"uri": "/helm_root/helm_micro/cleanup/",
	"title": "Limpar",
	"tags": [],
	"description": "",
	"content": "Para excluir o release do workshop, execute:\nhelm del --purge workshop  "
},
{
	"uri": "/conclusion/survey/",
	"title": "Deixe-nos saber o que você pensa!",
	"tags": [],
	"description": "",
	"content": " Por favor, faça nossa pesquisa! (function(t,e,s,n){var o,a,c;t.SMCX=t.SMCX||[],e.getElementById(n)||(o=e.getElementsByTagName(s),a=o[o.length-1],c=e.createElement(s),c.type=\"text/javascript\",c.async=!0,c.id=n,c.src=[\"https:\"===location.protocol?\"https://\":\"http://\",\"widget.surveymonkey.com/collect/website/js/tRaiETqnLgj758hTBazgd_2BU860jlhPrsKW9DSM0aec7fijRMWQEdDb7y2zM_2FUrIx.js\"].join(\"\"),a.parentNode.insertBefore(c,a))})(window,document,\"script\",\"smcx-sdk\");Create your own user feedback survey    "
},
{
	"uri": "/helm_root/helm_intro/",
	"title": "Instale o Helm em EKS",
	"tags": [],
	"description": "",
	"content": " Instale Helm no EKS Helm é um gerenciador de pacotes e uma ferramenta de gerenciamento de aplicativos para o Kubernetes que empacota vários recursos do Kubernetes em uma única unidade de implantação lógica chamada Chart.\nHelm ajuda você a:\n Conseguir um simples(um comando) implementações repetitivas Gerenciar a dependência de aplicativos, usando versões específicas de outros aplicativos e serviços Gerenciar várias configurações de implantação: test, staging, production e outros Executar trabalhos de pós/pré-implantação durante a implantação do aplicativo Atualizar / reverter e testar implantações de aplicativos  "
},
{
	"uri": "/calico/stars_policy_demo/",
	"title": "Demonstração de política do star",
	"tags": [],
	"description": "",
	"content": " Demonstração de política do stars Neste subcapítulo, criamos serviços frontend, backend, client e UI no cluster EKS e definimos políticas de rede para permitir ou bloquear a comunicação entre esses serviços. Essa demonstração também possui uma interface de gerenciamento que mostra os caminhos de entrada e saída disponíveis entre cada serviço.\n"
},
{
	"uri": "/batch/workflow-advanced/",
	"title": "Advanced Batch Workflow",
	"tags": [],
	"description": "",
	"content": " Advanced Batch Workflow Let\u0026rsquo;s take a look at a more complex workflow, involving passing artifacts between jobs, multiple dependencies, etc.\nSave the below manifest as teardrop.yaml using your favorite editor.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: teardrop- spec: entrypoint: teardrop templates: - name: create-chain container: image: alpine:latest command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;touch /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: whalesay inputs: parameters: - name: message artifacts: - name: chain path: /tmp/message container: image: docker/whalesay command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;echo Chain: ; cat /tmp/message* | sort | uniq | tee /tmp/message; cowsay This is Job {{inputs.parameters.message}}! ; echo {{inputs.parameters.message}} \u0026gt;\u0026gt; /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: whalesay-reduce inputs: parameters: - name: message artifacts: - name: chain-0 path: /tmp/message.0 - name: chain-1 path: /tmp/message.1 container: image: docker/whalesay command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;echo Chain: ; cat /tmp/message* | sort | uniq | tee /tmp/message; cowsay This is Job {{inputs.parameters.message}}! ; echo {{inputs.parameters.message}} \u0026gt;\u0026gt; /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: teardrop dag: tasks: - name: create-chain template: create-chain - name: Alpha dependencies: [create-chain] template: whalesay arguments: parameters: [{name: message, value: Alpha}] artifacts: - name: chain from: \u0026quot;{{tasks.create-chain.outputs.artifacts.chain}}\u0026quot; - name: Bravo dependencies: [Alpha] template: whalesay arguments: parameters: [{name: message, value: Bravo}] artifacts: - name: chain from: \u0026quot;{{tasks.Alpha.outputs.artifacts.chain}}\u0026quot; - name: Charlie dependencies: [Alpha] template: whalesay arguments: parameters: [{name: message, value: Charlie}] artifacts: - name: chain from: \u0026quot;{{tasks.Alpha.outputs.artifacts.chain}}\u0026quot; - name: Delta dependencies: [Bravo] template: whalesay arguments: parameters: [{name: message, value: Delta}] artifacts: - name: chain from: \u0026quot;{{tasks.Bravo.outputs.artifacts.chain}}\u0026quot; - name: Echo dependencies: [Bravo, Charlie] template: whalesay-reduce arguments: parameters: [{name: message, value: Echo}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Bravo.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Charlie.outputs.artifacts.chain}}\u0026quot; - name: Foxtrot dependencies: [Charlie] template: whalesay arguments: parameters: [{name: message, value: Foxtrot}] artifacts: - name: chain from: \u0026quot;{{tasks.create-chain.outputs.artifacts.chain}}\u0026quot; - name: Golf dependencies: [Delta, Echo] template: whalesay-reduce arguments: parameters: [{name: message, value: Golf}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Delta.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Echo.outputs.artifacts.chain}}\u0026quot; - name: Hotel dependencies: [Echo, Foxtrot] template: whalesay-reduce arguments: parameters: [{name: message, value: Hotel}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Echo.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Foxtrot.outputs.artifacts.chain}}\u0026quot;  This workflow uses a Directed Acyclic Graph (DAG) to explicitly define job dependencies. Each job in the workflow calls a whalesay template and passes a parameter with a unique name. Some jobs call a whalesay-reduce template which accepts multiple artifacts and combines them into a single artifact.\nEach job in the workflow pulls the artifact(s) and lists them in the \u0026ldquo;Chain\u0026rdquo;, then calls whalesay for the current job. Each job will then have a list of the previous job dependency chain (list of all jobs that had to complete before current job could run).\nRun the workflow.\nargo submit --watch teardrop.yaml  Name: teardrop-jfg5w Namespace: default ServiceAccount: default Status: Succeeded Created: Sat Nov 17 16:01:42 -0500 (7 minutes ago) Started: Sat Nov 17 16:01:42 -0500 (7 minutes ago) Finished: Sat Nov 17 16:03:35 -0500 (5 minutes ago) Duration: 1 minute 53 seconds STEP PODNAME DURATION MESSAGE ✔ teardrop-jfg5w ├-✔ create-chain teardrop-jfg5w-3938249022 3s ├-✔ Alpha teardrop-jfg5w-3385521262 6s ├-✔ Bravo teardrop-jfg5w-1878939134 35s ├-✔ Charlie teardrop-jfg5w-3753534620 35s ├-✔ Foxtrot teardrop-jfg5w-2036090354 5s ├-✔ Delta teardrop-jfg5w-37094256 34s ├-✔ Echo teardrop-jfg5w-4165010455 31s ├-✔ Hotel teardrop-jfg5w-2342859904 4s └-✔ Golf teardrop-jfg5w-1687601882 30s  Continue to the Argo Dashboard to explore this model further.\n"
},
{
	"uri": "/servicemesh/cleanup/",
	"title": "Limpeza",
	"tags": [],
	"description": "",
	"content": "Para limpar, siga os passos abaixo.\nPara remover o processo de configuração de telemetria / port-forward de porta\nkubectl delete -f istio-telemetry.yaml  Para remover os serviços virtuais / regras de destino do aplicativo\nkubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl delete -f samples/bookinfo/networking/destination-rule-all.yaml  Para remover o gateway / aplicativo\nkubectl delete -f samples/bookinfo/networking/bookinfo-gateway.yaml kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml  Para remover o Istio\nkubectl delete -f istio.yaml kubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml kubectl delete namespace istio-system  "
},
{
	"uri": "/introduction/basics/concepts_objects_details_2/",
	"title": "Detalhe de objetos do K8s (2/2)",
	"tags": [],
	"description": "",
	"content": " ReplicaSet  Garante que um número definido de pods esteja sempre em execução  Job  Garante que um pod seja executado corretamente até a conclusão  Service  Mapeia um endereço IP fixo para um grupo lógico de pods  Label  Pares de chave/valor usados ​​para associação e filtragem  "
},
{
	"uri": "/helm_root/helm_nginx/",
	"title": "Deploy Nginx With Helm",
	"tags": [],
	"description": "",
	"content": " Deploy Nginx With Helm Neste capítulo, vamos aprofundar com o Helm e demonstrar como instalar o servidor web NGINX através dos seguintes passos:\n Atualize o Repositório do Chart   Pesquisar o repositório de charts   Adicione o repositório Bitnami   Instalar o bitnami/nginx   Limpar   "
},
{
	"uri": "/cleanup/",
	"title": "Limpar",
	"tags": [],
	"description": "",
	"content": " Limpar "
},
{
	"uri": "/batch/dashboard/",
	"title": "Argo Dashboard",
	"tags": [],
	"description": "",
	"content": " Argo Dashboard Argo UI lists the workflows and visualizes each workflow (very handy for our last workflow).\nTo connect, use the same proxy connection setup in Deploy the Official Kubernetes Dashboard.\n  Show me the command   kubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true \u0026amp;  This will start the proxy, listen on port 8080, listen on all interfaces, and will disable the filtering of non-localhost requests.\nThis command will continue to run in the background of the current terminal\u0026rsquo;s session.\nWe are disabling request filtering, a security feature that guards against XSRF attacks. This isn\u0026rsquo;t recommended for a production environment, but is useful for our dev environment.\n   To access the Argo Dashboard:\n In your Cloud9 environment, click Preview / Preview Running Application Scroll to the end of the URL and append:  /api/v1/namespaces/argo/services/argo-ui/proxy/  You will see the teardrop workflow from Advanced Batch Workflow. Click on it to see a visualization of the workflow.\nThe workflow should relatively look like a teardrop, and provide a live status for each job. Click on Hotel to see a summary of the Hotel job.\nThis details basic information about the job, and includes a link to the Logs. The Hotel job logs list the job dependency chain and the current whalesay, and should look similar to:\nChain: Alpha Bravo Charlie Echo Foxtrot ____________________ \u0026lt; This is Job Hotel! \u0026gt; -------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  Explore the other jobs in the workflow to see each job\u0026rsquo;s status and logs.\n"
},
{
	"uri": "/introduction/architecture/",
	"title": "Arquitetura do Kubernetes",
	"tags": [],
	"description": "",
	"content": "Nesta seção, abordaremos os seguintes tópicos:\n Visão de Arquitetura   Plano de controle   Plano de dados   Configuração de cluster do Kubernetes   "
},
{
	"uri": "/helm_root/helm_micro/",
	"title": "Implantar exemplos de microsserviços usando o Helm",
	"tags": [],
	"description": "",
	"content": " Implantar exemplos de microsserviços usando o Helm Neste capítulo, demonstraremos como implantar microsserviços usando um chart Helm personalizado, em vez de fazer tudo manualmente usandokubectl.\nPara obter informações detalhadas sobre como trabalhar com templates de charts, consulte o Helm docs\n"
},
{
	"uri": "/batch/cleanup/",
	"title": "Limpar",
	"tags": [],
	"description": "",
	"content": " Cleanup Delete all workflows argo delete --all  Remove Artifact Repository Bucket ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) aws s3 rb s3://batch-artifact-repository-${ACCOUNT_ID}/ --force  Remove permissions for Artifact Repository Bucket INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r .Stacks[].StackName | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) aws iam delete-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker  Undeploy Argo kubectl delete -n argo -f https://raw.githubusercontent.com/argoproj/argo/v2.2.1/manifests/install.yaml kubectl delete ns argo  Cleanup Kubernetes Job kubectl delete job/whalesay  "
},
{
	"uri": "/introduction/architecture/architecture_control_and_data_overview/",
	"title": "Visão de Arquitetura",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 kubectl--api controller--api scheduler--api api--kubelet1 api--etcd kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;  "
},
{
	"uri": "/introduction/eks/",
	"title": "Amazon EKS",
	"tags": [],
	"description": "",
	"content": "Nesta seção, abordaremos os seguintes tópicos:\n Fluxo de trabalho de criação de cluster do EKS   O que acontece quando você cria seu cluster EKS   Alto nível   Arquitetura EKS para Plano de Controle e Comunicação do Nó worker   Amazon EKS!   "
},
{
	"uri": "/deploy/cleanup/",
	"title": "Limpar os aplicativos",
	"tags": [],
	"description": "",
	"content": "Para excluir os recursos criados pelos aplicativos, devemos excluir os deployments de aplicativos:\nRemover implantação dos aplicativos:\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml  "
},
{
	"uri": "/conclusion/",
	"title": "Conclusão",
	"tags": [],
	"description": "",
	"content": " Conclusão "
},
{
	"uri": "/introduction/architecture/architecture_control/",
	"title": "Plano de controle",
	"tags": [],
	"description": "",
	"content": "graph TB kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end kubectl--api controller--api scheduler--api api--kubelet api--etcd classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;   Um ou mais servidores de API: ponto de entrada para REST / kubectl\n etcd: Armazenamento de chave/valor distribuído\n Controller-manager: Sempre avaliando o estado atual vs desejado\n Scheduler: Agendamentos de pods para worker nodes\n  Confira a documentação oficial do Kubernetes para uma explicação mais detalhada dos componentes do plano de controle.\n"
},
{
	"uri": "/helm_root/helm_nginx/updatecharts/",
	"title": "Atualize o Repositório do Chart",
	"tags": [],
	"description": "",
	"content": "Helm usa um formato de embalagem chamado Charts. Um Chart é uma coleção de arquivos que descrevem recursos do k8s.\nOs Charts podem ser simples, descrevendo algo como um servidor da Web independente (que é o que vamos criar), mas eles também podem ser mais complexos, por exemplo, um charts que representa uma stack completa de aplicativos da Web, incluindo servidores da Web, bancos de dados, proxys etc.\nEm vez de instalar os recursos do k8s manualmente por meio do kubectl, podemos usar o Helm para instalar charts pré-definidos mais rapidamente, com menos chances de erros de digitação ou outros erros do operador.\nQuando você instala o Helm, você recebe um repositório padrão de Charts do Repositório Oficial de Chart do Helm.\nEsta é uma lista muito dinâmica que sempre muda devido a atualizações e novas adições. Para manter a lista local de Helm atualizada com todas essas mudanças, precisamos executar ocasionalmente [atualização de repositório]comando (https://docs.helm.sh/helm/#helm-repo-update).\nPara atualizar a lista local de charts do Helm, execute:\nhelm repo update  E você deve ver algo semelhante a:\nAguarde enquanto pegamos as últimas novidades dos seus repositórios de charts... ...Obteve com sucesso uma atualização do \u0026quot;stable\u0026quot; chart Atualização do repositório concluída. ⎈ Feliz Helming!⎈  Em seguida, procuraremos o gráfico do servidor da Web NGINX.\n"
},
{
	"uri": "/introduction/architecture/architecture_worker/",
	"title": "Plano de dados",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 api--kubelet1 kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;   Feito de worker nodes\n kubelet: Atua como um canal entre o servidor da API e o node\n kube-proxy: Gerenciamento de tradução e roteamento de IP\n  Confira a documentação oficial do Kubernetes para uma explicação mais detalhada dos componentes do plano de dados.\n"
},
{
	"uri": "/introduction/architecture/cluster_setup_options/",
	"title": "Configuração de cluster do Kubernetes",
	"tags": [],
	"description": "",
	"content": "Além da solução gerenciada do Amazon EKS, há muitas ferramentas disponíveis para ajudar a inicializar e configurar um cluster Kubernetes autogerenciado. Eles incluem:\n Minikube – Desenvolvimento e Aprendizagem Kops – Aprendizagem, Desenvolvimento, Produção Kubeadm – Aprendizagem, Desenvolvimento, Produção Docker for Mac - Aprendizagem e Desenvolvimento  Juntamente com essas soluções de código aberto, há também muitas opções comerciais disponíveis.\nVamos dar uma olhada no Amazon EKS!\n"
},
{
	"uri": "/introduction/eks/eks_customers/",
	"title": "Fluxo de trabalho de criação de cluster do EKS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_control_plane/",
	"title": "O que acontece quando você cria seu cluster EKS",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_high_level/",
	"title": "Alto nível",
	"tags": [],
	"description": "",
	"content": "Assim que o seu cluster EKS estiver pronto, você receberá um endpoint da API e você poderá usar o Kubectl,uma ferramenta desenvolvida pela comunidade para interagir com seu cluster.\n"
},
{
	"uri": "/introduction/eks/eks_high_architecture/",
	"title": "Arquitetura EKS para Plano de Controle e Comunicação do Nó worker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/stay_tuned/",
	"title": "Amazon EKS!",
	"tags": [],
	"description": "",
	"content": "Fique ligado enquanto continuamos a jornada com o EKS no próximo módulo! Sempre faça perguntas! Sinta-se à vontade para perguntar pessoalmente durante este workshop, ou a qualquer momento no canal oficial do Kubernetes Slack acessível via http://slack.k8s.io/.\n"
},
{
	"uri": "/helm_root/helm_nginx/searchchart/",
	"title": "Pesquisar o repositório de charts",
	"tags": [],
	"description": "",
	"content": "Agora que a nossa lista de charts do repositório foi atualizada, podemos procurar por charts.\nPara listar todos de Charts:\nhelm search  Isso deve resultar em algo parecido com:\nNAME CHART VERSION APP VERSION DESCRIPTION stable/acs-engine-autoscaler 2.2.0 2.1.1 Scales worker... stable/aerospike 0.1.7 v3.14.1.2 A Helm chart... ...  Você pode ver na saída que ele despejou a lista de todos os charts que ele conhece. Em alguns casos, isso pode ser útil, mas uma pesquisa ainda mais útil envolveria um argumento de palavra-chave. Então, em seguida, vamos procurar apenas por NGINX:\nhelm search nginx  Isso resulta em:\nNAME CHART VERSION APP VERSION DESCRIPTION stable/nginx-ingress 0.31.0 0.20.0 An nginx Ingress ... stable/nginx-ldapauth-proxy 0.1.2 1.13.5 nginx proxy ... stable/nginx-lego 0.3.1 Chart for... stable/gcloud-endpoints 0.1.2 1 DEPRECATED Develop... ...  Esta nova lista de Charts é específica para nginx, porque passamos o argumento nginx para o comando de pesquisa.\n"
},
{
	"uri": "/helm_root/helm_nginx/addbitnamirepo/",
	"title": "Adicione o repositório Bitnami",
	"tags": [],
	"description": "",
	"content": "No último slide, vimos que o NGINX oferece muitos produtos diferentes através do repositório Helm Chart padrão, mas o servidor web autônomo NGINX não é um deles.\nDepois de uma rápida pesquisa na web, descobrimos que existe um gráfico para o servidor da Web autônomo NGINX disponível por meio do repositório Chart Bitnami.\nPara adicionar o repositório do Bitnami Chart à nossa lista local de gráficos pesquisáveis:\nadicionar repositório helm bitnami https://charts.bitnami.com/bitnami  Depois disso, podemos pesquisar todos Bitnami Charts:\nhelm search bitnami  O que resulta em:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/bitnami-common 0.0.3 0.0.1 Chart with... bitnami/apache 2.1.2 2.4.37 Chart for Apache... bitnami/cassandra 0.1.0 3.11.3 Apache Cassandra... ...  Procure novamente pelo NGINX:\nhelm search nginx  Agora estamos vendo mais opções do NGINX em todos os repositórios:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/nginx 1.1.2 1.14.1 Chart for the nginx server bitnami/nginx-ingress-controller 2.1.4 0.20.0 Chart for the nginx Ingress... stable/nginx-ingress 0.31.0 0.20.0 An nginx Ingress controller ...  Ou até mesmo pesquise o repositório Bitnami, apenas para NGINX:\nhelm search bitnami/nginx  O que limita isso ao NGINX do Bitnami:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/nginx 1.1.2 1.14.1 Chart for the nginx server bitnami/nginx-ingress-controller 2.1.4 0.20.0 Chart for the nginx Ingress...  Em ambas as duas últimas pesquisas, vemos\nbitnami/nginx  como um resultado de pesquisa. Esse é o que estamos procurando, então vamos usar o Helm para instalá-lo no cluster EKS.\n"
},
{
	"uri": "/helm_root/helm_nginx/installnginx/",
	"title": "Instalar o bitnami/nginx",
	"tags": [],
	"description": "",
	"content": " Instalando o chart do servidor Web NGINX autônomo Bitnami nos envolve usando o comando helm install command.\nQuando instalamos usando o Helm, precisamos fornecer um nome de implantação ou um nome aleatório será atribuído à implantação automaticamente.\nDesafio: Como você pode usar o Helm para implantar o chart bitnami/nginx?\nSUGESTÃO: Use o utilitário helm para instalar o chart bitnami/nginx e especifique o nome mywebserver para a implantação do Kubernetes. Consulte a documentação do helm install ou executar o comando helm install --help para descobrir a sintaxe\n  Expanda aqui para ver a solução   helm install --name mywebserver bitnami/nginx    Depois de executar este comando, a saída confirma os tipos de objetos k8s que foram criados como resultado:\nNAME: mywebserver LAST DEPLOYED: Tue Nov 13 19:55:25 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1beta1/Deployment NAME AGE mywebserver-nginx 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE mywebserver-nginx-85985c8466-tczst 0/1 ContainerCreating 0 0s ==\u0026gt; v1/Service NAME AGE mywebserver-nginx 0s  Nos seguintes exemplos de comandos do kubectl , pode levar um minuto ou dois para cada um desses objetos DESIRED e CURRENT valores para corresponder; se eles não corresponderem na primeira tentativa, aguarde alguns segundos e execute o comando novamente para verificar o status.\n O primeiro objeto mostrado nesta saída é um Deployment. Um objeto de implantação gerencia distribuições (e reversões) de diferentes versões de um aplicativo.\nVocê pode inspecionar esse objeto de implementação mais detalhadamente executando o seguinte comando:\nkubectl describe deployment mywebserver-nginx  O próximo objeto mostrado criado pelo gráfico é um Pod. Um Pod é um grupo de um ou mais contêineres.\nPara verificar se o objeto Pod foi implantado com sucesso, podemos executar o seguinte comando:\nkubectl get pods -l app=mywebserver-nginx  E você deve ver uma saída semelhante a:\nNAME READY STATUS RESTARTS AGE mywebserver-nginx-85985c8466-tczst 1/1 Running 0 10s  O terceiro objeto que este chart cria para nós é um Service O Serviço nos permite entrar em contato com esse servidor da Web NGINX pela Internet, por meio de um Elastic Load Balancer (ELB).\nPara obter o URL completo deste serviço, execute:\nkubectl get service mywebserver-nginx -o wide  Isso deve produzir algo semelhante a:\nNAME TYPE CLUSTER-IP EXTERNAL-IP mywebserver-nginx LoadBalancer 10.100.223.99 abc123.amazonaws.com  Copie o valor para EXTERNAL-IP, abra uma nova guia no seu navegador da Web e cole-a. Pode levar alguns minutos para que o ELB e seu nome DNS associado fiquem disponíveis; Se você receber um erro, aguarde um minuto e clique em Atualizar.\n Quando o Serviço fica on-line, você deve ver uma mensagem de boas-vindas semelhante a:\nParabéns! Agora você implantou com sucesso o servidor da Web autônomo NGINX em seu cluster EKS!\n"
},
{
	"uri": "/helm_root/helm_nginx/cleaningup/",
	"title": "Limpar",
	"tags": [],
	"description": "",
	"content": "Para remover todos os objetos que o Helm Chart criou, podemos usar Helm delete.\nAntes de excluí-lo, podemos verificar o que temos em execução por meio do Helm list command:\nhelm list  Você deve ver uma saída semelhante à abaixo, que mostra que o mywebserver está instalado:\nNAME REVISION UPDATED STATUS CHART APP VERSION mywebserver 1 Tue Nov 13 19:55:25 2018 DEPLOYED nginx-1.1.2 1.14.1  Foi muito divertido; Tivemos ótimos momentos enviando HTTP para frente e para trás, mas agora é hora de excluir essa implantação. Deletar:\nhelm delete --purge mywebserver  E você deve encontrar a saída:\nrelease \u0026quot;mywebserver\u0026quot; deleted  O kubectl também demonstrará que nossos pods e serviços não estão mais disponíveis:\nkubectl get pods -l app=mywebserver-nginx kubectl get service mywebserver-nginx -o wide  Como tentaria acessar o serviço através do navegador da web através de uma recarga de página.\nCom isso, a limpeza está completa.\n"
},
{
	"uri": "/tabs-example/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Tabs with regular text:  Tab 1 Tab 2  echo \"This is tab 1.\"  println \"This is tab 2.\"   $(function(){$(\"#tab\").tabs();}); Tabs with code blocks:  Tab 1 Tab 2  echo \u0026#34;This is tab 1.\u0026#34;   println \u0026#34;This is tab 2.\u0026#34;    $(function(){$(\"#tab-with-code\").tabs();}); Guias que mostram o processo de instalação:  eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#tab-installation\").tabs();}); Segundo conjunto de guias mostrando o processo de instalação:  eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#more-tab-installation\").tabs();}); "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/authors/",
	"title": "Créditos",
	"tags": [],
	"description": "",
	"content": "###Agradecemos aos nossos colaboradores maravilhosos  para fazer do Open Source um lugar melhor!\n.ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start } .ghContributors  div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors  div label{ padding-left: 4px ; } .ghContributors  div span{ font-size: x-small; padding-left: 4px ; }   @brentley 50 commits   @geremyCohen 26 commits   @oak2278 20 commits   @dalbhanj 10 commits   @rnzsgh 9 commits   @jpeddicord 6 commits   @buzzsurfr 6 commits   @nikipat 4 commits   @gmarchand 3 commits   @tabern 3 commits   @ramnar 3 commits   @alexpulver 2 commits   @alexei-led 2 commits   @DocValerian 2 commits   @rasensio 2 commits   @waynekhan 2 commits   @johnstanfield 2 commits   @ocxo 1 commits   @sharmaanshul2102 1 commits   @bhean 1 commits   @ChanceLee0111 1 commits   @rebelthor 1 commits   @deki 1 commits   @enghwa 1 commits   @feanil 1 commits   @GElkayam 1 commits   @geordan-hatech 1 commits   @itaysk 1 commits   @techsolx 1 commits   @jmferrer 1 commits   @jupp0r 1 commits   @kimsaabyepedersen 1 commits   @Lixxia 1 commits   @LiranBri 1 commits   @mreferre 1 commits   @mikesigs 1 commits   @nihooge 1 commits   @wolruf 1 commits   @ranrotx 1 commits   @shaiss 1 commits   @jennylover 1 commits   @sajee 1 commits   @obbaeiei 1 commits   @trevorrobertsjr 1 commits   @vjsikha 1 commits   @Oik741 1 commits   @algestam 1 commits   @auziel 1 commits   @barelnir 1 commits   @gearora 1 commits   @jaybarnes 1 commits   "
},
{
	"uri": "/tabs-example/tabs/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tabs-example/tabs/eks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n"
},
{
	"uri": "/tabs-example/tabs/launcheks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n "
},
{
	"uri": "/example_cf_templates/",
	"title": "Example of using CloudFormation Templates",
	"tags": [],
	"description": "",
	"content": " Clique abaixo para adicionar uma stack do CloudFormation    Use these templates:       Template 1 example  Launch    Download     Template 2 example  Launch    Download     Template 3 example  Launch    Download      "
},
{
	"uri": "/prerequisites/self_paced/eu-west-1/",
	"title": "Ireland",
	"tags": [],
	"description": "",
	"content": "Crie um ambiente Cloud9: https://eu-west-1.console.aws.amazon.com/cloud9/home?region=eu-west-1\n"
},
{
	"uri": "/more_resources/",
	"title": "More Resources",
	"tags": [],
	"description": "",
	"content": " Descubra mais recursos da AWS para criar e executar seu aplicativo na AWS:\nMais oficinas  Amazon ECS Workshop - Aprenda a usar o Stelligent Mu para implantar uma arquitetura de microsserviço que é executada no AWS Fargate Amazon Lightsail Workshop - Se você está começando com a nuvem e procurando uma maneira de executar um ambiente de custo extremamente baixo, o Lightsail é perfeito. Aprenda como implantar no Amazon Lightsail com este workshop.  Ferramentas para AWS Fargate e Amazon ECS  Containers on AWS - Aprenda práticas recomendadas comuns para a execução de contêineres no AWS fargate - Ferramenta de linha de comando para interagir com o AWS Fargate. Com apenas um único comando, você pode criar, enviar e iniciar seu contêiner em Fargate, orquestrado pela ECS. Terraform - Use Terraform para implantar seus contêineres Docker em Fargate Wonqa é uma ferramenta para girar ambientes de controle de qualidade descartáveis ​​no AWS Fargate, com SSL ativado por Let\u0026rsquo;s Encrypt. Mais detalhes sobre Wonqa em Wonder Engineering blog coldbrew - Ferramenta fantástica que provisiona a infraestrutura do ECS, cria e implanta seu contêiner e conecta seus serviços a um balanceador de carga de aplicativos automaticamente. Tem uma ótima experiência de desenvolvedor para uso no dia a dia mu - Automatiza tudo relacionado aos devops do ECS e CI / CD. Essa estrutura permite que você escreva um arquivo de metadados simples e construa toda a infraestrutura necessária para que você possa implantar na ECS simplesmente enviando para o repositório do Git.  Courses  Microservices with Docker, Flask, and React - Aprenda a construir, testar e implantar microsserviços desenvolvidos pelo Docker, Flask, React Amazon ECS!  "
},
{
	"uri": "/prerequisites/self_paced/us-east-2/",
	"title": "Ohio",
	"tags": [],
	"description": "",
	"content": "Crie um ambiente Cloud9: https://us-east-2.console.aws.amazon.com/cloud9/home?region=us-east-2\n"
},
{
	"uri": "/prerequisites/self_paced/us-west-2/",
	"title": "Oregon",
	"tags": [],
	"description": "",
	"content": "Crie um ambiente Cloud9: https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]